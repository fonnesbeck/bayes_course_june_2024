{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/fonnesbeck/bayes_course_dec_2023/blob/master/notebooks/Section3_2-Non_Parametric_Models.ipynb)\n",
    "\n",
    "# Non-Parametric Models"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spline Models\n",
    "\n",
    "Often, the model we want to fit is not a perfect line between some $x$ and $y$.\n",
    "Instead, the parameters of the model are expected to vary over $x$.\n",
    "There are multiple ways to handle this situation, one of which is to fit a *spline*.\n",
    "Spline fit is effectively a sum of multiple individual curves (piecewise polynomials), each fit to a different section of $x$, that are tied together at their boundaries, often called *knots*.\n",
    "\n",
    "The spline is effectively multiple individual lines, each fit to a different section of $x$, that are tied together at their boundaries, often called *knots*.\n",
    "\n",
    "Below is a full working example of how to fit a spline using PyMC. The data and model are taken from [*Statistical Rethinking* 2e](https://xcelab.net/rm/statistical-rethinking/) by Richard McElreath.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import pandas as pd \n",
    "import pymc as pm\n",
    "import arviz as az\n",
    "import seaborn as sns\n",
    "import pytensor\n",
    "import pytensor.tensor as pt\n",
    "import matplotlib.pylab as plt\n",
    "import matplotlib.cm as cmap\n",
    "sns.set_context('notebook')\n",
    "\n",
    "RANDOM_SEED = 42\n",
    "DATA_URL = 'https://raw.githubusercontent.com/fonnesbeck/probabilistic_programming_pymc/master/data/'\n",
    "\n",
    "from patsy import dmatrix"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cherry blossom data\n",
    "\n",
    "The data for this example is the number of days (`doy` for \"days of year\") that the cherry trees were in bloom in each year (`year`). \n",
    "For convenience, years missing a `doy` were dropped (which is a bad idea to deal with missing data in general!)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    blossom_data = pd.read_csv(Path(\"..\", \"data\", \"cherry_blossoms.csv\"), sep=\";\")\n",
    "except FileNotFoundError:\n",
    "    blossom_data = pd.read_csv(pm.get_data(\"cherry_blossoms.csv\"), sep=\";\")\n",
    "\n",
    "\n",
    "blossom_data.dropna().describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "blossom_data = blossom_data.dropna(subset=[\"doy\"]).reset_index(drop=True)\n",
    "blossom_data.head(n=10)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After dropping rows with missing data, there are 827 years with the numbers of days in which the trees were in bloom."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "blossom_data.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we visualize the data, it is clear that there a lot of annual variation, but some evidence for a non-linear trend in bloom days over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "blossom_data.plot.scatter(\n",
    "    \"year\", \"doy\", color=\"cornflowerblue\", s=10, title=\"Cherry Blossom Data\", ylabel=\"Days in bloom\"\n",
    ");"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The model\n",
    "\n",
    "We will fit the following model.\n",
    "\n",
    "$D \\sim \\mathcal{N}(\\mu, \\sigma)$  \n",
    "$\\quad \\mu = a + Bw$  \n",
    "$\\qquad a \\sim \\mathcal{N}(100, 10)$  \n",
    "$\\qquad w \\sim \\mathcal{N}(0, 10)$  \n",
    "$\\quad \\sigma \\sim \\text{Exp}(1)$\n",
    "\n",
    "The number of days of bloom $D$ will be modeled as a normal distribution with mean $\\mu$ and standard deviation $\\sigma$. In turn, the mean will be a linear model composed of a y-intercept $a$ and spline defined by the basis $B$ multiplied by the model parameter $w$ with a variable for each region of the basis. Both have relatively weak normal priors.\n",
    "\n",
    "### Prepare the spline\n",
    "\n",
    "The spline will have 15 *knots*, splitting the year into 16 sections (including the regions covering the years before and after those in which we have data). The knots are the boundaries of the spline, the name owing to how the individual lines will be tied together at these boundaries to make a continuous and smooth curve.  The knots will be unevenly spaced over the years such that each region will have the same proportion of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_knots = 15\n",
    "knot_list = np.quantile(blossom_data.year, np.linspace(0, 1, num_knots))\n",
    "knot_list"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is a plot of the locations of the knots over the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "blossom_data.plot.scatter(\n",
    "    \"year\", \"doy\", color=\"cornflowerblue\", s=10, title=\"Cherry Blossom Data\", ylabel=\"Day of Year\"\n",
    ")\n",
    "for knot in knot_list:\n",
    "    plt.gca().axvline(knot, color=\"grey\", alpha=0.4);"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use `patsy` to create the matrix $B$ that will be the b-spline basis for the regression.\n",
    "The degree is set to 3 to create a cubic b-spline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "B = dmatrix(\n",
    "    \"bs(year, knots=knots, degree=3, include_intercept=True) - 1\",\n",
    "    {\"year\": blossom_data.year.values, \"knots\": knot_list[1:-1]},\n",
    ")\n",
    "B"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The b-spline basis is plotted below, showing the *domain* of each piece of the spline. The height of each curve indicates how influential the corresponding model covariate (one per spline region) will be on model's inference of that region. The overlapping regions represent the knots, showing how the smooth transition from one region to the next is formed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spline_df = (\n",
    "    pd.DataFrame(B)\n",
    "    .assign(year=blossom_data.year.values)\n",
    "    .melt(\"year\", var_name=\"spline_i\", value_name=\"value\")\n",
    ")\n",
    "\n",
    "color = plt.cm.magma(np.linspace(0, 0.80, len(spline_df.spline_i.unique())))\n",
    "\n",
    "fig = plt.figure()\n",
    "for i, c in enumerate(color):\n",
    "    subset = spline_df.query(f\"spline_i == {i}\")\n",
    "    subset.plot(\"year\", \"value\", c=c, ax=plt.gca(), label=i)\n",
    "plt.legend(title=\"Spline Index\", loc=\"upper center\", fontsize=8, ncol=6);"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit the model\n",
    "\n",
    "Finally, the model can be built using PyMC. A graphical diagram shows the organization of the model parameters (note that this requires the installation of `python-graphviz`, which I recommend doing in a `conda` virtual environment)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.asarray(B, order=\"F\").shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "COORDS = {\"splines\": np.arange(B.shape[1]),\n",
    "          \"obs\": np.arange(blossom_data.shape[0])}\n",
    "with pm.Model(coords=COORDS) as spline_model:\n",
    "    \n",
    "    a = pm.Normal(\"a\", 100, 5)\n",
    "    w = pm.Normal(\"w\", mu=0, sigma=3, size=B.shape[1], dims=\"splines\")\n",
    "    mu = pm.Deterministic(\"mu\", a + pm.math.dot(np.asarray(B, order=\"F\"), w.T))\n",
    "    sigma = pm.Exponential(\"sigma\", 1)\n",
    "    D = pm.Normal(\"D\", mu=mu, sigma=sigma, observed=blossom_data.doy, dims=\"obs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pm.model_to_graphviz(spline_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with spline_model:\n",
    "    trace = pm.sample_prior_predictive()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "az.plot_ppc(trace, group=\"prior\", kind='cumulative');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with spline_model:\n",
    "    trace.extend(pm.sample(draws=1000, tune=1000, random_seed=RANDOM_SEED, chains=4))\n",
    "    pm.sample_posterior_predictive(trace, extend_inferencedata=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis\n",
    "\n",
    "Now we can analyze the draws from the posterior of the model.\n",
    "\n",
    "### Parameter Estimates\n",
    "\n",
    "Below is a table summarizing the posterior distributions of the model parameters.\n",
    "The posteriors of $a$ and $\\sigma$ are quite narrow while those for $w$ are wider.\n",
    "This is likely because all of the data points are used to estimate $a$ and $\\sigma$ whereas only a subset are used for each value of $w$.\n",
    "(It could be interesting to model these hierarchically allowing for the sharing of information and adding regularization across the spline.) \n",
    "The effective sample size and $\\widehat{R}$ values all look good, indicating that the model has converged and sampled well from the posterior distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "az.plot_energy(trace);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "az.summary(trace, var_names=[\"a\", \"w\", \"sigma\"])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The trace plots of the model parameters look good (homogeneous and no sign of trend), further indicating that the chains converged and mixed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "az.plot_trace(trace, var_names=[\"a\", \"w\", \"sigma\"]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "az.plot_forest(trace, var_names=[\"w\"], r_hat=True);"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another visualization of the fit spline values is to plot them multiplied against the basis matrix.\n",
    "The knot boundaries are shown as vertical lines again, but now the spline basis is multiplied against the values of $w$ (represented as the rainbow-colored curves). The dot product of $B$ and $w$ – the actual computation in the linear model – is shown in black."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wp = trace.posterior[\"w\"].mean((\"chain\", \"draw\")).values\n",
    "\n",
    "spline_df = (\n",
    "    pd.DataFrame(B * wp.T)\n",
    "    .assign(year=blossom_data.year.values)\n",
    "    .melt(\"year\", var_name=\"spline_i\", value_name=\"value\")\n",
    ")\n",
    "\n",
    "spline_df_merged = (\n",
    "    pd.DataFrame(np.dot(B, wp.T))\n",
    "    .assign(year=blossom_data.year.values)\n",
    "    .melt(\"year\", var_name=\"spline_i\", value_name=\"value\")\n",
    ")\n",
    "\n",
    "\n",
    "color = plt.cm.rainbow(np.linspace(0, 1, len(spline_df.spline_i.unique())))\n",
    "fig = plt.figure()\n",
    "for i, c in enumerate(color):\n",
    "    subset = spline_df.query(f\"spline_i == {i}\")\n",
    "    subset.plot(\"year\", \"value\", c=c, ax=plt.gca(), label=i)\n",
    "spline_df_merged.plot(\"year\", \"value\", c=\"black\", lw=2, ax=plt.gca())\n",
    "plt.legend(title=\"Spline Index\", loc=\"lower center\", fontsize=8, ncol=6)\n",
    "\n",
    "for knot in knot_list:\n",
    "    plt.gca().axvline(knot, color=\"grey\", alpha=0.4);"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model predictions\n",
    "\n",
    "Lastly, we can visualize the predictions of the model using the posterior predictive check."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "post_pred = az.summary(trace, var_names=[\"mu\"]).reset_index(drop=True)\n",
    "blossom_data_post = blossom_data.copy().reset_index(drop=True)\n",
    "blossom_data_post[\"pred_mean\"] = post_pred[\"mean\"]\n",
    "blossom_data_post[\"pred_hdi_lower\"] = post_pred[\"hdi_3%\"]\n",
    "blossom_data_post[\"pred_hdi_upper\"] = post_pred[\"hdi_97%\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "blossom_data.plot.scatter(\n",
    "    \"year\",\n",
    "    \"doy\",\n",
    "    color=\"cornflowerblue\",\n",
    "    s=10,\n",
    "    title=\"Cherry blossom data with posterior predictions\",\n",
    "    ylabel=\"Days in bloom\",\n",
    ")\n",
    "for knot in knot_list:\n",
    "    plt.gca().axvline(knot, color=\"grey\", alpha=0.4)\n",
    "\n",
    "blossom_data_post.plot(\"year\", \"pred_mean\", ax=plt.gca(), lw=3, color=\"firebrick\")\n",
    "plt.fill_between(\n",
    "    blossom_data_post.year,\n",
    "    blossom_data_post.pred_hdi_lower,\n",
    "    blossom_data_post.pred_hdi_upper,\n",
    "    color=\"firebrick\",\n",
    "    alpha=0.4,\n",
    ");"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gaussian Processes\n",
    "\n",
    "Use of the term \"non-parametric\" in the context of Bayesian analysis is something of a misnomer. This is because the first and fundamental step in Bayesian modeling is to specify a *full probability model* for the problem at hand. It is rather difficult to explicitly state a full probability model without the use of probability functions, which are parametric. Bayesian non-parametric methods do not imply that there are no parameters, but rather that the number of parameters grows with the size of the dataset. In fact, Bayesian non-parametric models are *infinitely* parametric."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building models with Gaussians\n",
    "\n",
    "What if we chose to use Gaussian distributions to model our data? \n",
    "\n",
    "$$p(x \\mid \\pi, \\Sigma) = (2\\pi)^{-k/2}|\\Sigma|^{-1/2} \\exp\\left\\{ -\\frac{1}{2} (x-\\mu)^{\\prime}\\Sigma^{-1}(x-\\mu) \\right\\}$$\n",
    "\n",
    "There would not seem to be an advantage to doing this, because normal distributions are not particularly flexible distributions in and of themselves. However, adopting a set of Gaussians (a multivariate normal vector) confers a number of advantages. First, the marginal distribution of any subset of elements from  a multivariate normal distribution is also normal:\n",
    "\n",
    "$$p(x,y) = \\mathcal{N}\\left(\\left[{\n",
    "\\begin{array}{c}\n",
    "  {\\mu_x}  \\\\\n",
    "  {\\mu_y}  \\\\\n",
    "\\end{array}\n",
    "}\\right], \\left[{\n",
    "\\begin{array}{c}\n",
    "  {\\Sigma_x} & {\\Sigma_{xy}}  \\\\\n",
    "  {\\Sigma_{xy}^T} & {\\Sigma_y}  \\\\\n",
    "\\end{array}\n",
    "}\\right]\\right)$$\n",
    "\n",
    "$$p(x) = \\int p(x,y) dy = \\mathcal{N}(\\mu_x, \\Sigma_x)$$\n",
    "\n",
    "Also, conditionals distributions of a subset of a multivariate normal distribution (conditional on the remaining elements) are normal too:\n",
    "\n",
    "$$p(x|y) = \\mathcal{N}(\\mu_x + \\Sigma_{xy}\\Sigma_y^{-1}(y-\\mu_y), \n",
    "\\Sigma_x-\\Sigma_{xy}\\Sigma_y^{-1}\\Sigma_{xy}^T)$$\n",
    "\n",
    "A Gaussian process generalizes the multivariate normal to infinite dimension. It is defined as an infinite collection of random variables, any finite subset of which have a Gaussian distribution. Thus, the marginalization property is explicit in its definition. Another way of thinking about an infinite vector is as a *function*. When we write a function that takes continuous values as inputs, we are essentially specifying an infinte vector that only returns values (indexed by the inputs) when the function is called upon to do so. By the same token, this notion of an infinite-dimensional Gaussian as a function allows us to work with them computationally: we are never required to store all the elements of the Gaussian process, only to calculate them on demand.\n",
    "\n",
    "So, we can describe a Gaussian process as a ***disribution over functions***. Just as a multivariate normal distribution is completely specified by a mean vector and covariance matrix, a GP is fully specified by a **mean function** and a **covariance function**:\n",
    "\n",
    "$$p(x) \\sim \\mathcal{GP}(m(x), k(x,x^{\\prime}))$$\n",
    "\n",
    "It is the marginalization property that makes working with a Gaussian process feasible: we can marginalize over the infinitely-many variables that we are not interested in, or have not observed. \n",
    "\n",
    "For example, one specification of a GP might be as follows:\n",
    "\n",
    "$$\\begin{aligned}\n",
    "m(x) &=0 \\\\\n",
    "k(x,x^{\\prime}) &= \\theta_1\\exp\\left(-\\frac{\\theta_2}{2}(x-x^{\\prime})^2\\right)\n",
    "\\end{aligned}$$\n",
    "\n",
    "here, the covariance function is a **squared exponential**, for which values of $x$ and $x^{\\prime}$ that are close together result in values of $k$ closer to 1 and those that are far apart return values closer to zero. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def exponential_cov(x, y, scale, length_scale):\n",
    "    return scale * np.exp( -0.5 * length_scale * np.subtract.outer(x, y)**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12,5))\n",
    "xrange = np.linspace(0, 5)\n",
    "ax1.plot(xrange, exponential_cov(0, xrange, 1, 1))\n",
    "ax1.set_xlabel('x')\n",
    "ax1.set_ylabel('cov(0, x)')\n",
    "\n",
    "z = np.array([exponential_cov(xrange, xprime, 1, 1) for xprime in xrange])\n",
    "ax2.imshow(z, cmap=\"inferno\", \n",
    "       interpolation='none', \n",
    "       extent=(0, 5, 5, 0))\n",
    "ax2.set_xlabel('x')\n",
    "ax2.set_ylabel('x')\n",
    "\n",
    "plt.tight_layout();"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It may seem odd to simply adopt the zero function to represent the mean function of the Gaussian process -- surely we can do better than that! It turns out that most of the learning in the GP involves the covariance function and its parameters, so very little is gained in specifying a complicated mean function.\n",
    "\n",
    "For a finite number of points, the GP becomes a multivariate normal, with the mean and covariance as the mean functon and covariance function evaluated at those points."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sampling from a Gaussian Process Prior\n",
    "\n",
    "To make this notion of a \"distribution over functions\" more concrete, let's quickly demonstrate how we obtain realizations from a Gaussian process, which result in an evaluation of a function over a set of points. All we will do here is sample from the *prior* Gaussian process, so before any data have been introduced. What we need first is our covariance function, which will be the squared exponential, and a function to evaluate the covariance at given points (resulting in a covariance matrix)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going generate realizations sequentially, point by point, using the lovely conditioning property of mutlivariate Gaussian distributions. Here is that conditional:\n",
    "\n",
    "$$p(y^*| x^*, y, x) = \\mathcal{N}(\\Sigma_{x^*x}\\Sigma_y^{-1}y,\\>\n",
    "\\Sigma_{x^*}-\\Sigma_{x^*x}\\Sigma_y^{-1}\\Sigma_{x^*x}^T)$$\n",
    "\n",
    "And this the function that implements it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conditional(x_new, x, y, scale, length_scale):\n",
    "    B = exponential_cov(x_new, x, scale, length_scale)\n",
    "    C = exponential_cov(x, x, scale, length_scale)\n",
    "    A = exponential_cov(x_new, x_new, scale, length_scale)\n",
    "    mu = np.linalg.inv(C).dot(B.T).T.dot(y)\n",
    "    sigma = A - B.dot(np.linalg.inv(C).dot(B.T))\n",
    "    return(mu.squeeze(), sigma.squeeze())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will start with a Gaussian process prior with hyperparameters $\\theta_0=1, \\theta_1=10$. We will also assume a zero function as the mean, so we can plot a band that represents one standard deviation from the mean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scale, length_scale = 1, 10\n",
    "sigma_0 = exponential_cov(0, 0, scale, length_scale)\n",
    "xpts = np.arange(-3, 3, step=0.01)\n",
    "plt.errorbar(xpts, np.zeros(len(xpts)), yerr=sigma_0, capsize=0)\n",
    "plt.ylim(-3, 3);"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's select an arbitrary starting point to sample, say $x=1$. Since there are no prevous points, we can sample from an unconditional Gaussian:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = [1.]\n",
    "y = [np.random.normal(scale=sigma_0)]\n",
    "y"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now update our confidence band, given the point that we just sampled, using the covariance function to generate new point-wise intervals, conditional on the value $[x_0, y_0]$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sigma_1 = exponential_cov(x, x, scale, length_scale)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(x, data, kernel, scale, length_scale, sigma, t):\n",
    "    k = [kernel(x, y, scale, length_scale) for y in data]\n",
    "    Sinv = np.linalg.inv(sigma)\n",
    "    y_pred = np.dot(k, Sinv).dot(t)\n",
    "    sigma_new = kernel(x, x, scale, length_scale) - np.dot(k, Sinv).dot(k)\n",
    "    return y_pred, sigma_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_pred = np.linspace(-3, 3, 1000)\n",
    "predictions = [predict(i, x, exponential_cov, scale, length_scale, sigma_1, y) for i in x_pred]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred, sigmas = np.transpose(predictions)\n",
    "plt.errorbar(x_pred, y_pred, yerr=sigmas, capsize=0)\n",
    "plt.plot(x, y, \"ro\")\n",
    "plt.xlim(-3, 3); plt.ylim(-3, 3);"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So conditional on this point, and the covariance structure we have specified, we have essentially constrained the probable location of additional points. Let's now sample another:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m, s = conditional([-0.7], x, y, scale, length_scale)\n",
    "y2 = np.random.normal(m, s)\n",
    "y2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This point is added to the realization, and can be used to further update the location of the next point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.append(-0.7)\n",
    "y.append(y2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sigma_2 = exponential_cov(x, x, scale, length_scale)\n",
    "\n",
    "predictions = [predict(i, x, exponential_cov, scale, length_scale, sigma_2, y) for i in x_pred]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred, sigmas = np.transpose(predictions)\n",
    "plt.errorbar(x_pred, y_pred, yerr=sigmas, capsize=0)\n",
    "plt.plot(x, y, \"ro\")\n",
    "plt.xlim(-3, 3); plt.ylim(-3, 3);"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of course, sampling sequentially is just a heuristic to demonstrate how the covariance structure works. We can just as easily sample several points at once:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_more = [-2.1, -1.5, 0.3, 1.8, 2.5]\n",
    "mu, s = conditional(x_more, x, y, scale, length_scale)\n",
    "y_more = np.random.multivariate_normal(mu, s)\n",
    "y_more"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x += x_more\n",
    "y += y_more.tolist()\n",
    "\n",
    "sigma_new = exponential_cov(x, x, scale, length_scale)\n",
    "\n",
    "predictions = [predict(i, x, exponential_cov, scale, length_scale, sigma_new, y) for i in x_pred]\n",
    "\n",
    "y_pred, sigmas = np.transpose(predictions)\n",
    "plt.errorbar(x_pred, y_pred, yerr=sigmas, capsize=0)\n",
    "plt.plot(x, y, \"ro\")\n",
    "plt.ylim(-3, 3);"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So as the density of points becomes high, the result will be one realization (function) from the prior GP. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "\n",
    "Generate a new draw from this GP, using a different set of points. (*HINT* just create new input values for `x` and and use them with `conditional`.)\n",
    "\n",
    "Generate another draw, this time use a different set of hyperparameters: $\\theta_0=1, \\theta_1=1$. And another with $\\theta_0=0.1, \\theta_1=1$. How do the functions differ? What is the effect of the hyperparameters on the function space?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def sample_gp_prior(scale, length_scale):\n",
    "\n",
    "    x_ = list(np.arange(-2.5, 2.5, step=0.2))\n",
    "    mu, s = conditional(x_, [], [], scale, length_scale)\n",
    "    y_ = np.random.multivariate_normal(mu, s)\n",
    "\n",
    "    sigma_new = exponential_cov(x_, x_, scale, length_scale)\n",
    "\n",
    "    predictions = [predict(i, x_, exponential_cov, scale, length_scale, sigma_new, y_) for i in x_pred]\n",
    "\n",
    "    y_pred, sigmas = np.transpose(predictions)\n",
    "    plt.errorbar(x_pred, y_pred, yerr=sigmas, capsize=0)\n",
    "    plt.plot(x_, y_, \"ro\")\n",
    "    plt.ylim(-3, 3);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wrute answer here"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This example, of course, is trivial because it is simply a random function drawn from the prior. What we are really interested in is *learning* about an underlying function from information residing in our data. In a parametric setting, we either specify a likelihood, which we then maximize with respect to the parameters, of a full probability model, for which we calculate the posterior in a Bayesian context. Though the integrals associated with posterior distributions are typically intractable for parametric models, they do not pose a problem with Gaussian processes."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gaussian processes regression\n",
    "\n",
    "The following simulated data clearly shows some type of non-linear process, corrupted by a certain amount of observation or measurement error so it should be a reasonable task for a Gaussian process approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the seed\n",
    "np.random.seed(1)\n",
    "\n",
    "n = 100 # The number of data points\n",
    "x = np.linspace(0, 10, n)\n",
    "X = x[:, None] # The inputs to the GP, they must be arranged as a column vector\n",
    "\n",
    "# Define the true covariance function and its parameters\n",
    "l_true = 1.0\n",
    "eta_true = 3.0\n",
    "cov_func = eta_true**2 * pm.gp.cov.Matern52(1, l_true)\n",
    "\n",
    "# A mean function that is zero everywhere\n",
    "mean_func = pm.gp.mean.Zero()\n",
    "\n",
    "# The latent function values are one sample from a multivariate normal\n",
    "# Note that we have to call `eval()` because PyMC built on top of PyTensor\n",
    "f_true = np.random.multivariate_normal(mean_func(X).eval(), \n",
    "    cov_func(X).eval() + 1e-8*np.eye(n), 1).flatten()\n",
    "\n",
    "# The observed data is the latent function plus a small amount of IID Gaussian noise\n",
    "# The standard deviation of the noise is `sigma`\n",
    "sigma_true = 2.0\n",
    "y = f_true + sigma_true * np.random.randn(n)\n",
    "\n",
    "## Plot the data and the unobserved latent function\n",
    "fig = plt.figure(figsize=(12,5)); ax = fig.gca()\n",
    "ax.plot(X, f_true, \"dodgerblue\", lw=3, label=\"True f\");\n",
    "ax.plot(X, y, 'ok', ms=3, alpha=0.5, label=\"Data\");\n",
    "ax.set_xlabel(\"X\"); ax.set_ylabel(\"y\"); plt.legend();"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Marginal Likelihood Implementation\n",
    "\n",
    "The `gp.Marginal` class in PyMC implements the simplest case of GP regression:  the observed data are the sum of a GP and Gaussian noise.  `gp.Marginal` has a `marginal_likelihood` method, a `conditional` method, and a `predict` method.  Given a mean and covariance function, the function $f(x)$ is modeled as,\n",
    "\n",
    "$$\n",
    "f(x) \\sim \\mathcal{GP}(m(x),\\, k(x, x')) \\,.\n",
    "$$\n",
    "\n",
    "The observations $y$ are the unknown function plus noise\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "  \\epsilon &\\sim N(0, \\Sigma) \\\\\n",
    "  y &= f(x) + \\epsilon \\\\\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "### The Marginal Likelihood\n",
    "\n",
    "The marginal likelihood is the normalizing constant for the posterior distribution, and is the integral of the product of the likelihood and prior.\n",
    "\n",
    "$$p(y|X) = \\int_f p(y|f,X)p(f|X) df$$\n",
    "\n",
    "where for Gaussian processes, we are marginalizing over function values $f$ (instead of parameters $\\theta$).\n",
    "\n",
    "**GP prior**:\n",
    "\n",
    "$$\\log p(f|X) = - \\frac{k}{2}\\log2\\pi - \\frac{1}{2}\\log|K| -\\frac{1}{2}f^TK^{-1}f $$\n",
    "\n",
    "**Gaussian likelihood**:\n",
    "\n",
    "$$\\log p(y|f,X) = - \\frac{k}{2}\\log2\\pi - \\frac{1}{2}\\log|\\sigma^2I| -\\frac{1}{2}(y-f)^T(\\sigma^2I)^{-1}(y-f) $$\n",
    "\n",
    "**Marginal likelihood**:\n",
    "\n",
    "$$\\log p(y|X) = - \\frac{k}{2}\\log2\\pi - \\frac{1}{2}\\log|K + \\sigma^2I| - \\frac{1}{2}y^T(K+\\sigma^2I)^{-1}y $$\n",
    "\n",
    "Notice that the marginal likelihood includes both a data fit term $- \\frac{1}{2}y^T(K+\\sigma^2I)^{-1}y$ and a parameter penalty term $\\frac{1}{2}\\log|K + \\sigma^2I|$. Hence, the marginal likelihood can help us select an appropriate covariance function, based on its fit to the dataset at hand.\n",
    "\n",
    "### Choosing parameters\n",
    "\n",
    "This is relevant because we have to make choices regarding the parameters of our Gaussian process; they were chosen arbitrarily for the random functions we demonstrated above.\n",
    "\n",
    "For example, in the squared exponential covariance function, we must choose two parameters:\n",
    "\n",
    "$$k(x,x^{\\prime}) = \\theta_1\\exp\\left(-\\frac{\\theta_2}{2}(x-x^{\\prime})^2\\right)$$\n",
    "\n",
    "The first parameter $\\theta_1$ is a scale parameter, which allows the function to yield values outside of the unit interval. The second parameter $\\theta_2$ is a length scale parameter that determines the degree of covariance between $x$ and $x^{\\prime}$; smaller values will tend to smooth the function relative to larger values.\n",
    "\n",
    "We can use the **marginal likelihood** to select appropriate values for these parameters, since it trades off model fit with model complexity. Thus, an optimization procedure can be used to select values for $\\theta$ that maximize the marginial likelihood."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Covariance functions\n",
    "\n",
    "The behavior of individual realizations from the GP is governed by the covariance function. This function controls both the degree of *shrinkage* to the mean function and the *smoothness* of functions sampled from the GP.\n",
    "\n",
    "PyMC includes a library of covariance functions to choose from. A flexible choice to start with is the Mat&#232;rn covariance. \n",
    "\n",
    "$$k_{M}(x) = \\frac{\\sigma^2}{\\Gamma(\\nu)2^{\\nu-1}} \\left(\\frac{\\sqrt{2 \\nu} x}{l}\\right)^{\\nu} K_{\\nu}\\left(\\frac{\\sqrt{2 \\nu} x}{l}\\right)$$\n",
    "\n",
    "where where $\\Gamma$ is the gamma function and $K$ is a modified Bessel function. The form of covariance matrices sampled from this function is governed by three parameters, each of which controls a property of the covariance.\n",
    "\n",
    "* **amplitude** ($\\sigma$) controls the scaling of the output along the y-axis. This parameter is just a scalar multiplier, and is therefore usually left out of implementations of the Mat&#232;rn function (*i.e.* set to one)\n",
    "\n",
    "* **lengthscale** ($l$) complements the amplitude by scaling realizations on the x-axis. Larger values make points appear closer together.\n",
    "\n",
    "* **roughness** ($\\nu$) controls the sharpness of ridges in the covariance function, which ultimately affect the roughness (smoothness) of realizations.\n",
    "\n",
    "Though in general all the parameters are non-negative real-valued, when $\\nu = p + 1/2$ for integer-valued $p$, the function can be expressed partly as a polynomial function of order $p$ and generates realizations that are $p$-times differentiable, so values $\\nu \\in \\{3/2, 5/2\\}$ are extremely common."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To provide an idea regarding the variety of forms or covariance functions, here's small selection of available ones:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_grid = np.linspace(0,2,200)[:,None]\n",
    "\n",
    "# function to display covariance matrices\n",
    "def plot_cov(X, K, stationary=True):\n",
    "    K = K + 1e-8*np.eye(X.shape[0])\n",
    "    x = X.flatten()\n",
    "    \n",
    "    with sns.axes_style(\"white\"):\n",
    "\n",
    "        fig = plt.figure(figsize=(14,5))\n",
    "        ax1 = fig.add_subplot(121)\n",
    "        m = ax1.imshow(K, cmap=\"inferno\", \n",
    "                       interpolation='none', \n",
    "                       extent=(np.min(X), np.max(X), np.max(X), np.min(X))); \n",
    "        plt.colorbar(m);\n",
    "        ax1.set_title(\"Covariance Matrix\")\n",
    "        ax1.set_xlabel(\"X\")\n",
    "        ax1.set_ylabel(\"X\")\n",
    "\n",
    "        ax2 = fig.add_subplot(122)\n",
    "        if not stationary:\n",
    "            ax2.plot(x, np.diag(K), \"k\", lw=2, alpha=0.8)\n",
    "            ax2.set_title(\"The Diagonal of K\")\n",
    "            ax2.set_ylabel(\"k(x,x)\")\n",
    "        else:\n",
    "            ax2.plot(x, K[:,0], \"k\", lw=2, alpha=0.8)\n",
    "            ax2.set_title(\"K as a function of x - x'\")\n",
    "            ax2.set_ylabel(\"k(x,x')\")\n",
    "        ax2.set_xlabel(\"X\")\n",
    "\n",
    "        fig = plt.figure(figsize=(14,4))\n",
    "        ax = fig.add_subplot(111)\n",
    "        samples = np.random.multivariate_normal(np.zeros(200), K, 5).T;\n",
    "        for i in range(samples.shape[1]):\n",
    "            ax.plot(x, samples[:,i], color=cmap.inferno(i*0.2), lw=2);\n",
    "        ax.set_title(\"Samples from GP Prior\")\n",
    "        ax.set_xlabel(\"X\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quadratic exponential covariance\n",
    "\n",
    "This is the squared exponential covariance function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pm.Model() as model:\n",
    "    l = 0.2 \n",
    "    tau = 2.0\n",
    "    b = 0.5\n",
    "    cov = b + tau * pm.gp.cov.ExpQuad(1, l)\n",
    "\n",
    "K = pytensor.function([], cov(X_grid))()\n",
    "\n",
    "plot_cov(X_grid, K)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Matern $\\nu=3/2$ covariance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pm.Model() as model:\n",
    "    l = 0.2\n",
    "    tau = 2.0\n",
    "    cov = tau * pm.gp.cov.Matern12(1, l)\n",
    "\n",
    "K = pytensor.function([], cov(X_grid))()\n",
    "\n",
    "plot_cov(X_grid, K)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cosine covariance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pm.Model() as model:\n",
    "    l = 0.2\n",
    "    tau = 2.0\n",
    "    cov = tau * pm.gp.cov.Cosine(1, l)\n",
    "\n",
    "K = pytensor.function([], cov(X_grid))()\n",
    "\n",
    "plot_cov(X_grid, K)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise: Combining Kernels\n",
    "\n",
    "Modeling with a single kernel is useful if your data is all the same type, but what if you have multiple types of features, but you still want to model them together? \n",
    "\n",
    "It turns out that we can build a kernel over different datatypes by multiplying or adding kernels together.\n",
    "\n",
    "Choose a couple of kernels from above and create a covariance function that multiplies them together. Plot the result with `plot_cov`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your answer here"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare this to the result when the kernels are added instead of multiplied."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your answer here"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have a general idea about covariance functions, let's begin by defining one for our first model.\n",
    "\n",
    "We can use a Matern(5/2) covariance to model our simulated data, and pass this as the `cov_func` argument to the `Marginal` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pm.Model() as model:\n",
    "    \n",
    "    l = pm.Gamma(\"l\", alpha=2, beta=1)\n",
    "    eta = pm.HalfCauchy(\"eta\", beta=5)\n",
    "    \n",
    "    cov = eta**2 * pm.gp.cov.Matern52(1, l)\n",
    "    mean = pm.gp.mean.Constant(c=1)\n",
    "    gp = pm.gp.Marginal(mean_func=mean, cov_func=cov)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The `.marginal_likelihood` method\n",
    "\n",
    "The unknown latent function can be analytically integrated out of the product of the GP prior probability with a normal likelihood.  This quantity is called the marginal likelihood. \n",
    "\n",
    "$$\n",
    "p(y \\mid x) = \\int p(y \\mid f, x) \\, p(f \\mid x) \\, df\n",
    "$$\n",
    "\n",
    "The log of the marginal likelihood, $p(y \\mid x)$, is\n",
    "\n",
    "$$\n",
    "\\log p(y \\mid x) = \n",
    "  -\\frac{1}{2} (\\mathbf{y} - \\mathbf{m}_x)^{T} \n",
    "               (\\mathbf{K}_{xx} + \\boldsymbol\\Sigma)^{-1} \n",
    "               (\\mathbf{y} - \\mathbf{m}_x)\n",
    "  - \\frac{1}{2}|\\mathbf{K}_{xx} + \\boldsymbol\\Sigma|\n",
    "  - \\frac{n}{2}\\log (2 \\pi)\n",
    "$$\n",
    "\n",
    "$\\boldsymbol\\Sigma$ is the covariance matrix of the Gaussian noise.  Since the Gaussian noise doesn't need to be white to be conjugate, the `marginal_likelihood` method supports either using a white noise term when a scalar is provided, or a noise covariance function when a covariance function is provided.\n",
    "\n",
    "The `gp.marginal_likelihood` method implements the quantity given above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = x.reshape(-1, 1)\n",
    "\n",
    "with model:\n",
    "    \n",
    "    sigma = pm.HalfCauchy(\"sigma\", beta=5)\n",
    "    obs = gp.marginal_likelihood(\"obs\", X=X, y=y, sigma=sigma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with model:\n",
    "    marginal_post = pm.sample(500, tune=2000, nuts_sampler='numpyro', chains=1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can collect the results into a pandas dataframe to display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary = az.summary(marginal_post, var_names=[\"l\", \"eta\", \"sigma\"], round_to=2, kind=\"stats\")\n",
    "summary[\"True value\"] = [l_true, eta_true, sigma_true]\n",
    "summary"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The `.conditional` distribution\n",
    "\n",
    "In addition to fitting the model, we would like to be able to generate predictions. This implies sampling from the posterior predictive distribution, which if you recall is just some linear algebra:\n",
    "\n",
    "$$\\begin{aligned}\n",
    "m^*(x^*) &= k(x^*,x)^T[k(x,x) + \\sigma^2I]^{-1}y \\\\\n",
    "k^*(x^*) &= k(x^*,x^*)+\\sigma^2 - k(x^*,x)^T[k(x,x) + \\sigma^2I]^{-1}k(x^*,x)\n",
    "\\end{aligned}$$\n",
    "\n",
    "PyMC allows for predictive sampling after the model is fit, using the recorded values of the model parameters to generate samples. The `conditional` method implements the predictive GP above, called with a grid of points over which to generate realizations:\n",
    "\n",
    "The `.conditional` has an optional flag for `pred_noise`, which defaults to `False`.  When `pred_noise=False`, the `conditional` method produces the predictive distribution for the underlying function represented by the GP.  When `pred_noise=True`, the `conditional` method produces the predictive distribution for the GP plus noise.  \n",
    "\n",
    "If using an additive GP model, the conditional distribution for individual components can be constructed by setting the optional argument `given`.  "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can define a grid of new values from `x=0` to `x=20`, then add the GP conditional to the model, given the new X values:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_new = np.linspace(0, 20, 600)[:,None]\n",
    "\n",
    "with model:\n",
    "    f_pred = gp.conditional(\"f_pred\", X_new)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can draw samples from the posterior predictive distribution over the specified grid of values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with model:\n",
    "    pred_samples = pm.sample_posterior_predictive(\n",
    "        marginal_post.sel(draw=slice(0, 20)), var_names=['f_pred']\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the results\n",
    "fig = plt.figure(figsize=(12,5)); ax = fig.gca()\n",
    "\n",
    "# plot the samples from the gp posterior with samples and shading\n",
    "from pymc.gp.util import plot_gp_dist\n",
    "f_pred_samples = az.extract(pred_samples, group=\"posterior_predictive\", var_names=[\"f_pred\"])\n",
    "plot_gp_dist(ax, samples=f_pred_samples.T, x=X_new);\n",
    "\n",
    "# plot the data and the true latent function\n",
    "plt.plot(x, f_true, \"dodgerblue\", lw=3, label=\"True f\");\n",
    "plt.plot(x, y, 'ok', ms=3, alpha=0.5, label=\"Observed data\");\n",
    "\n",
    "# axis labels and title\n",
    "plt.xlabel(\"X\"); plt.ylim([-13,13]);\n",
    "plt.title(\"Posterior distribution over $f(x)$ at the observed values\"); plt.legend();"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The prediction also matches the results from `gp.Latent` very closely.  What about predicting new data points?  Here we only predicted $f_*$, not $f_*$ + noise, which is what we actually observe.\n",
    "\n",
    "The `conditional` method of `gp.Marginal` contains the flag `pred_noise` whose default value is `False`.  To draw from the *posterior predictive* distribution, we simply set this flag to `True`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with model:\n",
    "    y_pred = gp.conditional(\"y_pred\", X_new, pred_noise=True)\n",
    "    y_samples = pm.sample_posterior_predictive(marginal_post.sel(draw=slice(0, 20)), var_names=['y_pred'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(12,5)); ax = fig.gca()\n",
    "\n",
    "# posterior predictive distribution\n",
    "y_pred_samples = az.extract(y_samples, group=\"posterior_predictive\", var_names=[\"y_pred\"])\n",
    "plot_gp_dist(ax, y_pred_samples.T, X_new, plot_samples=False, palette=\"bone_r\");\n",
    "\n",
    "# overlay a scatter of one draw of random points from the \n",
    "#   posterior predictive distribution\n",
    "plt.plot(X_new, y_pred_samples.values.T[1], \"co\", ms=2, label=\"Predicted data\");\n",
    "\n",
    "# plot original data and true function\n",
    "plt.plot(X, y, 'ok', ms=3, alpha=1.0, label=\"observed data\");\n",
    "plt.plot(X, f_true, \"dodgerblue\", lw=3, label=\"true f\");\n",
    "\n",
    "plt.xlabel(\"x\"); plt.ylim([-13,13]);\n",
    "plt.title(\"posterior predictive distribution, y_*\"); plt.legend();"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Real-world example: Spawning salmon\n",
    "\n",
    "That was contrived data; let's try applying Gaussian processes to a real problem. The plot below shows the relationship between the number of spawning salmon in a particular stream and the number of fry that are recruited into the population in the spring.\n",
    "\n",
    "We would like to model this relationship, which appears to be non-linear (we have biological knowledge that suggests it should be non-linear too).\n",
    "\n",
    "![](images/spawn.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    salmon_data = pd.read_table('../data/salmon.txt', sep='\\s+', index_col=0)\n",
    "except FileNotFoundError:\n",
    "    salmon_data = pd.read_table(DATA_URL + 'salmon.txt', sep='\\s+', index_col=0)\n",
    "    \n",
    "salmon_data.plot.scatter(x='spawners', y='recruits', s=50);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pm.Model() as salmon_model:\n",
    "\n",
    "    # Lengthscale\n",
    "    rho = pm.LogNormal('rho', 0, 1)\n",
    "    eta = pm.LogNormal('eta', 0, 1)\n",
    "    \n",
    "    M = pm.gp.mean.Linear(coeffs=(salmon_data.recruits/salmon_data.spawners).mean())\n",
    "    K = (eta**2) * pm.gp.cov.ExpQuad(1, rho) \n",
    "    \n",
    "    sigma = pm.HalfNormal('sigma', 5)\n",
    "    \n",
    "    recruit_gp = pm.gp.Marginal(mean_func=M, cov_func=K)\n",
    "    recruit_gp.marginal_likelihood('recruits', X=salmon_data.spawners.values.reshape(-1,1), y=salmon_data.recruits.values, noise=sigma)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with salmon_model:\n",
    "    salmon_trace = pm.sample(1000, tune=2000, nuts_sampler='numpyro', chains=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "az.plot_trace(salmon_trace, var_names=['rho', 'eta', 'sigma']);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_pred = np.linspace(0, 500, 100).reshape(-1, 1)\n",
    "with salmon_model:\n",
    "    salmon_pred = recruit_gp.conditional(\"salmon_pred\", X_pred)\n",
    "    salmon_samples = pm.sample_posterior_predictive(salmon_trace.sel(draw=slice(0, 5)), var_names=[\"salmon_pred\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = salmon_data.plot.scatter(x='spawners', y='recruits', c='k', s=50)\n",
    "ax.set_ylim(0, None)\n",
    "for x in az.extract(salmon_samples.posterior_predictive)['salmon_pred'].values.T:\n",
    "    ax.plot(X_pred, x, alpha=0.5, c='r');"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "\n",
    "We might be interested in what may happen if the population gets very large -- say, 600 or 800 spawners. We can predict this, though it goes well outside the range of data that we have observed. Generate predictions from the posterior predictive distribution (via `conditional`) that covers this range of spawners."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write answer here"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using `.predict`\n",
    "\n",
    "We can use the `.predict` method to return the mean and variance given a particular `point`.  Since we used `find_MAP` in this example, `predict` returns the same mean and covariance that the distribution of `.conditional` has.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict\n",
    "with model:\n",
    "    mu, var = gp.predict(X_new, point=az.extract(marginal_post.posterior.sel(draw=[0])).squeeze(), diag=True)\n",
    "sd = np.sqrt(var)\n",
    "\n",
    "# draw plot\n",
    "fig = plt.figure(figsize=(12,5)); ax = fig.gca()\n",
    "\n",
    "# plot mean and 2sigma intervals\n",
    "plt.plot(X_new, mu, 'r', lw=2, label=\"mean and 2sigma region\");\n",
    "plt.plot(X_new, mu + 2*sd, 'r', lw=1); plt.plot(X_new, mu - 2*sd, 'r', lw=1);\n",
    "plt.fill_between(X_new.flatten(), mu - 2*sd, mu + 2*sd, color=\"r\", alpha=0.5)\n",
    "\n",
    "# plot original data and true function\n",
    "plt.plot(X, y, 'ok', ms=3, alpha=1.0, label=\"observed data\");\n",
    "plt.plot(X, f_true, \"dodgerblue\", lw=3, label=\"true f\");\n",
    "\n",
    "plt.xlabel(\"x\"); plt.ylim([-13,13]);\n",
    "plt.title(\"predictive mean and 2sigma interval\"); plt.legend();"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Latent Variable Implementation\n",
    "\n",
    "The `gp.Latent` class is a more general implementation of a GP.  It is called \"Latent\" because the underlying function values are treated as latent variables.  It has a `prior` method, and a `conditional` method.  Given a mean and covariance function, the function $f(x)$ is modeled as,\n",
    "\n",
    "$$\n",
    "f(x) \\sim \\mathcal{GP}(m(x),\\, k(x, x')) \\,.\n",
    "$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `.prior`\n",
    "\n",
    "With some data set of finite size, the `prior` method places a multivariate normal prior distribution on the vector of function values, $\\mathbf{f}$,\n",
    "\n",
    "$$\n",
    "\\mathbf{f} \\sim \\text{MvNormal}(\\mathbf{m}_{x},\\, \\mathbf{K}_{xx}) \\,,\n",
    "$$\n",
    "\n",
    "where the vector $\\mathbf{m}$ and the matrix $\\mathbf{K}_{xx}$ are the mean vector and covariance matrix evaluated over the inputs $x$.  \n",
    "\n",
    "By default, PyMC reparameterizes the prior on `f` under the hood by rotating it with the Cholesky factor of its covariance matrix.  This helps to reduce covariances in the posterior of the transformed random variable, `v`.  The reparameterized model is,\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "  \\mathbf{v} \\sim \\text{N}(0, 1)& \\\\\n",
    "  \\mathbf{L} = \\text{Cholesky}(\\mathbf{K}_{xx})& \\\\\n",
    "  \\mathbf{f} = \\mathbf{m}_{x} + \\mathbf{Lv} \\\\\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "This reparameterization can be disabled by setting the optional flag in the `prior` method, `reparameterize = False`.  The default is `True`."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Robust regession\n",
    "\n",
    "The following is an example showing how to specify a simple model with a GP prior using the `gp.Latent` class.  So we can verify that the inference we perform is correct, the data set is made using a draw from a GP.  This will be identical to the first example, except that the noise is Student-T distributed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the seed\n",
    "np.random.seed(1)\n",
    "\n",
    "n = 100 # The number of data points\n",
    "X = np.linspace(0, 10, n)[:, None] # The inputs to the GP, they must be arranged as a column vector\n",
    "\n",
    "# Define the true covariance function and its parameters\n",
    "ls_true = 1.0\n",
    "eta_true = 3.0\n",
    "cov_func = eta_true**2 * pm.gp.cov.Matern52(1, ls_true)\n",
    "\n",
    "# A mean function that is zero everywhere\n",
    "mean_func = pm.gp.mean.Zero()\n",
    "\n",
    "# The latent function values are one sample from a multivariate normal\n",
    "# Note that we have to call `eval()` because PyMC built on top of PyTensor\n",
    "f_true = np.random.multivariate_normal(mean_func(X).eval(), \n",
    "                                       cov_func(X).eval() + 1e-8*np.eye(n), 1).flatten()\n",
    "\n",
    "# The observed data is the latent function plus a small amount of T distributed noise\n",
    "# The standard deviation of the noise is `sigma`, and the degrees of freedom is `nu`\n",
    "sigma_true = 2.0\n",
    "nu_true = 3.0\n",
    "y = f_true + sigma_true * np.random.standard_t(nu_true, size=n)\n",
    "\n",
    "## Plot the data and the unobserved latent function\n",
    "fig = plt.figure(figsize=(12,5)); ax = fig.gca()\n",
    "ax.plot(X, f_true, \"dodgerblue\", lw=3, label=\"True f\");\n",
    "ax.plot(X, y, 'ok', ms=3, label=\"Data\");\n",
    "ax.set_xlabel(\"X\"); ax.set_ylabel(\"y\"); plt.legend();"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's the model in PyMC.  We use a $\\text{Gamma}(2, 1)$ prior over the lengthscale parameter, and weakly informative $\\text{HalfCauchy}(2)$ priors over the covariance function scale, and noise scale.  A $\\text{Gamma}(2, 0.1)$ prior is assigned to the degrees of freedom parameter of the noise.  Finally, a GP prior is placed on the unknown function.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pm.Model() as model:\n",
    "    ls = pm.Gamma(\"ls\", alpha=2, beta=1)\n",
    "    eta = pm.HalfCauchy(\"eta\", beta=2)\n",
    "    \n",
    "    cov = eta**2 * pm.gp.cov.Matern52(1, ls)\n",
    "    gp = pm.gp.Latent(cov_func=cov)\n",
    "    \n",
    "    f = gp.prior(\"f\", X=X)\n",
    "    \n",
    "    sigma = pm.HalfCauchy(\"sigma\", beta=2)\n",
    "    nu = pm.Gamma(\"nu\", alpha=2, beta=0.1)\n",
    "    y_ = pm.StudentT(\"y\", mu=f, lam=1.0/sigma, nu=nu, observed=y)\n",
    "    \n",
    "    trace = pm.sample(nuts_sampler='numpyro', chains=1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below are the posteriors of the covariance function hyperparameters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "az.plot_trace(trace, var_names=[\"eta\", \"sigma\", \"ls\", \"nu\"], backend_kwargs=dict(constrained_layout=True));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the results\n",
    "fig = plt.figure(figsize=(12,5)); ax = fig.gca()\n",
    "\n",
    "# plot the samples from the gp posterior with samples and shading\n",
    "from pymc.gp.util import plot_gp_dist\n",
    "plot_gp_dist(ax, trace.posterior[\"f\"].values[0], X);\n",
    "\n",
    "# plot the data and the true latent function\n",
    "plt.plot(X, f_true, \"dodgerblue\", lw=3, label=\"True f\");\n",
    "plt.plot(X, y, 'ok', ms=3, alpha=0.5, label=\"Observed data\");\n",
    "\n",
    "# axis labels and title\n",
    "plt.xlabel(\"X\"); plt.ylabel(\"True f(x)\"); \n",
    "plt.title(\"Posterior distribution over $f(x)$ at the observed values\"); plt.legend();"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Faster Gaussian processes\n",
    "\n",
    "One of the major constraints that limits the utility of Gaussian processes in practice is the inversion of $K$ when calculating the posterior covariance. Since it is evaluated at every observed data point, its execution time is $\\mathcal{O(n^3)}$, which makes Gaussian processes (in the form I have presented here) impractical for larger datasets.\n",
    "\n",
    "An approach for dealing with this computation complexity is to look for an approximation to accelerate training and prediction. For Gaussian processes, this can be accomplished by employing a **sparse approximation** to the Gram matrix that places $M<<N$ *inducing points* along the range of the input variables, and uses this to estimate the full covariance matrix for the observed points. \n",
    "\n",
    "The `gp.MarginalSparse` class implements sparse, or inducing point, GP approximations.  It works identically to `gp.Marginal`, except it additionally requires the locations of the inducing points (denoted `Xu`), and it accepts the argument `sigma` instead of `noise` because these sparse approximations assume white IID noise.\n",
    "\n",
    "The downside of sparse approximations is that they reduce the expressiveness of the GP.  Reducing the dimension of the covariance matrix effectively reduces the number of covariance matrix eigenvectors that can be used to fit the data.  \n",
    "\n",
    "A choice that needs to be made is where to place the inducing points.  One option is to use a subset of the inputs.  Another possibility is to use K-means.  The location of the inducing points can also be an unknown and optimized as part of the model.  These sparse approximations are useful for speeding up calculations when the density of data points is high and the lengthscales is larger than the separations between inducing points. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dense dataset\n",
    "\n",
    "For the following examples, we use the same data set as was used in the `gp.Marginal` example, but with more data points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the seed\n",
    "np.random.seed(42)\n",
    "\n",
    "n = 2000 # The number of data points\n",
    "X = 10*np.sort(np.random.rand(n))[:,None]\n",
    "\n",
    "# Define the true covariance function and its parameters\n",
    "ls_true = 1.0\n",
    "eta_true = 3.0\n",
    "cov_func = eta_true**2 * pm.gp.cov.Matern52(1, ls_true)\n",
    "\n",
    "# A mean function that is zero everywhere\n",
    "mean_func = pm.gp.mean.Zero()\n",
    "\n",
    "# The latent function values are one sample from a multivariate normal\n",
    "# Note that we have to call `eval()` because PyMC built on top of PyTensor\n",
    "f_true = np.random.multivariate_normal(mean_func(X).eval(), \n",
    "                                       cov_func(X).eval() + 1e-8*np.eye(n), 1).flatten()\n",
    "\n",
    "# The observed data is the latent function plus a small amount of IID Gaussian noise\n",
    "# The standard deviation of the noise is `sigma`\n",
    "sigma_true = 2.0\n",
    "y = f_true + sigma_true * np.random.randn(n)\n",
    "\n",
    "## Plot the data and the unobserved latent function\n",
    "fig = plt.figure(figsize=(12,5)); ax = fig.gca()\n",
    "ax.plot(X, f_true, \"dodgerblue\", lw=3, label=\"True f\");\n",
    "ax.plot(X, y, 'ok', ms=3, alpha=0.5, label=\"Data\");\n",
    "ax.set_xlabel(\"X\"); ax.set_ylabel(\"The true f(x)\"); plt.legend();"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can specify a sparse marginal likelihood model via `MarginalSparse`, where the approximation method can be chosen. We will use the **fully independent training conditional (FITC)** algorithm to estimate the model, with the critical approximation being the imposition of a conditional independence assumption on the joint prior over training and test cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pm.Model() as sparse_model:\n",
    "    \n",
    "    ls = pm.Gamma(\"ls\", alpha=2, beta=1)\n",
    "    eta = pm.HalfCauchy(\"eta\", beta=5)\n",
    "    \n",
    "    cov = eta**2 * pm.gp.cov.ExpQuad(1, ls)\n",
    "    gp = pm.gp.MarginalApprox(cov_func=cov, approx=\"FITC\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We next need a set of inducing points. We will initialize 20 inducing points with the **K-means** algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with sparse_model:\n",
    "    \n",
    "    Xu = pm.gp.util.kmeans_inducing_points(20, X)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From here, we use the `marginal_likelihood` method, just as we did with the full GP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with sparse_model:\n",
    "    \n",
    "    sigma = pm.HalfCauchy(\"sigma\", 5)\n",
    "    obs = gp.marginal_likelihood(\"obs\", X=X, Xu=Xu, y=y, noise=sigma, jitter=1e-5)\n",
    "    \n",
    "    trace = pm.sample(nuts_sampler='numpyro', chains=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "az.plot_trace(trace, var_names=[\"eta\", \"ls\"], backend_kwargs=dict(constrained_layout=True));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_new = np.linspace(-1, 11, 200)[:,None]\n",
    "\n",
    "# add the GP conditional to the model, given the new X values\n",
    "with sparse_model:\n",
    "    f_pred = gp.conditional(\"f_pred\", X_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with sparse_model:\n",
    "    pred_samples = pm.sample_posterior_predictive(trace.sel(draw=slice(0, 5)), var_names=[\"f_pred\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the results\n",
    "fig = plt.figure(figsize=(12,5)); ax = fig.gca()\n",
    "\n",
    "# plot the samples from the gp posterior with samples and shading\n",
    "from pymc.gp.util import plot_gp_dist\n",
    "f_pred_samples = az.extract(pred_samples, group=\"posterior_predictive\", var_names=[\"f_pred\"])\n",
    "plot_gp_dist(ax, f_pred_samples.T, X_new);\n",
    "\n",
    "# plot the data and the true latent function\n",
    "plt.plot(X, y, 'ok', ms=3, alpha=0.5, label=\"Observed data\");\n",
    "plt.plot(X, f_true, \"dodgerblue\", lw=3, label=\"True f\");\n",
    "plt.plot(Xu, 10*np.ones(Xu.shape[0]), \"co\", ms=10, label=\"Inducing point locations\")\n",
    "\n",
    "# axis labels and title\n",
    "plt.xlabel(\"X\"); plt.ylim([-13,13]);\n",
    "plt.title(\"Posterior distribution over $f(x)$ at the observed values\"); plt.legend();"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HSGP\n",
    "\n",
    "HSGP is a drop in replacement for `pm.gp.Latent`, or the vanilla GP.  There isn't a need to fuss with marginalizing it out, not sampling the GP directly and trying to get it back later.  The only thing we need to choose are `m` and `c` (or `L`).  `m` controls the number of basis vectors to use, and `c` or `L` control the boundary where the approximation is accurate.  You can read `c` as the multiple on the half range of the data over which the approximation is accurate.  For instance, if your `x` data goes between -5 and 5, and you want the GP approximation to be accurate from -10 to 10, you'll need to choose `c` to be *at least* 2.\n",
    "\n",
    "Some rules of thumb are:\n",
    "\n",
    "- Larger `m` means we can use smaller lengthscales (more wiggly)\n",
    "- Larger `c` means we can use larger lengthscales\n",
    "\n",
    "In [Ruitort-Mayol et. al.'s paper](https://arxiv.org/pdf/2004.11408.pdf), they give some recommendations for choosing `m` and `c` that we can plot below.  "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cherry blossom data set\n",
    "\n",
    "Let's look at the cherry blossom dataset again, notably three columns: \n",
    "- `doy` the day of year of the first bloom\n",
    "- `temp` the temperature (since it goes so far back in time, I'm not sure how it's reconstructed)\n",
    "- `year` is of course the year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(pm.get_data(\"cherry_blossoms.csv\"), sep=\";\")\n",
    "df = df[[\"year\", \"doy\", \"temp\"]].dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(2, 1, figsize=(10, 7)); axs = axs.flatten()\n",
    "\n",
    "axs[0].scatter(df[\"year\"], df[\"temp\"])\n",
    "axs[1].scatter(df[\"year\"], df[\"doy\"])\n",
    "\n",
    "axs[1].set_xlabel(\"year\")\n",
    "axs[0].set_ylabel(\"temp (C)\")\n",
    "axs[1].set_ylabel(\"day of the first bloom\")\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.ticker import MultipleLocator\n",
    "\n",
    "c_list = np.array([1.2, 1.3, 1.5, 1.75, 2.0, 2.5, 3.0, 4.0, 5.0, 6.0])\n",
    "ell = np.linspace(1, 1000, 500)\n",
    "S = (df[\"year\"] - df[\"year\"].mean()).max() # half-range of the input data\n",
    "\n",
    "fig = plt.figure(figsize=(10, 5))\n",
    "ax = fig.gca()\n",
    "\n",
    "cmap = plt.cm.cividis\n",
    "colors = np.arange(len(c_list)) / len(c_list)\n",
    "\n",
    "for i, c in enumerate(c_list):\n",
    "    m = 2.65 * (c / ell) * S\n",
    "   \n",
    "    ix = c >= (4.1 * (ell / S))\n",
    "    m[~ix] = np.nan\n",
    "    ax.semilogy(ell, m, color=cmap(colors[i]), label=\"c = %s\" % str(c), lw=3);\n",
    "\n",
    "ax.grid(True);\n",
    "\n",
    "ax.xaxis.set_major_locator(MultipleLocator(50))\n",
    "ax.xaxis.set_minor_locator(MultipleLocator(10))\n",
    "\n",
    "ax.set_title(\"Matern52 HSGP approx parameter curves\");\n",
    "ax.set_ylim([10, 1000]);\n",
    "ax.set_xlabel(\"lengthscale (ℓ)\");\n",
    "ax.set_ylabel(\"number of basis vectors (m)\");\n",
    "ax.legend();"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The curves show the regions where the HSGP approximation is accurate.  For instance, the darkest blue line at $c=1.2$ means that if we choose $c=1.2$, our approximation will be valid for the smallest lengthscales up until about $\\ell = 130$.  The yellow curve at $c=6.0$ means shows that the approximation is accurate up until about $\\ell = 650$.  **How we choose these values depends on our prior for the lengthscale**.  In our case we are setting 95% of the prior mass between 20 and 200.  We'll be safe and choose $c=2$ and $m=400$ so our prior range is well covered pretty well.  \n",
    "\n",
    "Also, keep in mind that HSGP scales as $\\mathcal{O}(nm + m)$, so the smaller we can choose $m$, the better, speed-wise.  $c$ plays no role in computation speed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coords = {\n",
    "    \"year\": df.year.tolist()\n",
    "}\n",
    "with pm.Model(coords=coords) as model:\n",
    "    # Set prior on GP hyperparameters\n",
    "    eta = pm.Exponential(\"eta\", lam=0.25)\n",
    "    \n",
    "    ell_params = pm.find_constrained_prior(\n",
    "        pm.InverseGamma, \n",
    "        lower=20, upper=200, \n",
    "        init_guess={\"alpha\": 2, \"beta\": 20},\n",
    "        mass=0.95,\n",
    "    )\n",
    "    ell = pm.InverseGamma(\"ell\", **ell_params)\n",
    "    \n",
    "    # Specify covariance function and GP\n",
    "    cov = eta**2 * pm.gp.cov.Matern52(1, ls=ell)\n",
    "    gp = pm.gp.HSGP(m=[400], c=2.0, cov_func=cov)\n",
    "    f = gp.prior(\"f\", X=df[\"year\"].values[:, None], dims=\"year\")\n",
    "    \n",
    "    intercept = pm.Normal(\"intercept\", mu=df[\"temp\"].mean(), sigma=2 * df[\"temp\"].std())\n",
    "    mu = pm.Deterministic(\"mu\", intercept + f, dims=\"year\")\n",
    "    \n",
    "    sigma = pm.HalfNormal(\"sigma\", sigma=2 * df[\"temp\"].std())\n",
    "    pm.Normal(\"y\", mu=mu, sigma=sigma, observed=df[\"temp\"].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with model:\n",
    "    trace_gp = pm.sample(nuts_sampler='numpyro', chains=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "az.plot_trace(trace_gp, var_names=[\"eta\", \"ell\", \"sigma\", \"intercept\"]);"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This took a few minutes on my machine.  It's definitely slower than the spline models, but we obtained results that look much better.  Compare to when we used a spline with 400 knots.  NUTs depends a ton on the posterior geometry, and sampling the GP hyperparameters has a huge influence, but the speeds are still comparable.  We also see a lengthscale of about 40, which we can interpret as meaning that it takes about 40 years for significant changes in temperature to occur.  We can also interpret this to roughly mean that the GP is comfortable predicting out about 40 years.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(18, 10));\n",
    "\n",
    "f = az.extract(trace_gp, group=\"posterior\", var_names=\"mu\")\n",
    "pm.gp.util.plot_gp_dist(ax=ax, samples=f.values.T, x=df[\"year\"].values)\n",
    "ax.plot(df[\"year\"].values, df[\"temp\"].values, \"ok\", alpha=0.25);\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example: Nashville daily temperatures\n",
    "\n",
    "The file `TNNASHVI.txt` in your data directory contains daily temperature readings for Nashville, courtesy of the [Average Daily Temperature Archive](http://academic.udayton.edu/kissock/http/Weather/). This data, as one would expect, oscillates annually. Use a Gaussian process to fit a regression model to this data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "daily_temps = pd.read_table(\"../data/TNNASHVI.txt\", sep='\\s+', \n",
    "                            names=['month','day','year','temp'], na_values=-99)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temps_2010 = daily_temps.temp[daily_temps.year>2010]\n",
    "temps_2010.plot(style='b.', figsize=(10,6), grid=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write answer here"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## References\n",
    "\n",
    "[Rasmussen, C. E., & Williams, C. K. I. (2005). Gaussian Processes for Machine Learning (Adaptive Computation and Machine Learning series). The MIT Press.](http://www.amazon.com/books/dp/026218253X)\n",
    "\n",
    "[Quinonero-Candela, J. & Rasmussen, C. E. (2005). A Unifying View of Sparse Approximate Gaussian Process Regression\n",
    "Journal of Machine Learning Research 6, 1939–1959.](http://www.jmlr.org/papers/v6/quinonero-candela05a.html)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "_draft": {
   "nbviewer_url": "https://gist.github.com/05a9210d3ea6546e14e6fcb8fd10a906"
  },
  "gist": {
   "data": {
    "description": "GP Showdown.ipynb",
    "public": true
   },
   "id": "05a9210d3ea6546e14e6fcb8fd10a906"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  },
  "latex_envs": {
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 0
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
