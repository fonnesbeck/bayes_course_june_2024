{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/fonnesbeck/bayes_course_june_2024/blob/master/notebooks/Section2_1-MCMC.ipynb)\n",
    "\n",
    "# Lesson: Bayesian Linear Regression\n",
    "\n",
    "Now that we have covered the basics of Bayesian inference and Markov Chain Monte Carlo (MCMC) methods, we can apply these concepts to a specific class of statistical model, linear regression. Regression models are everywhere in data science, and Bayesian linear regression offers a powerful framework for incorporating prior knowledge and uncertainty into the modeling process.\n",
    "\n",
    "Bayesian linear regression extends the traditional linear regression framework by incorporating prior beliefs about the parameters and updating these beliefs with data to return a posterior distribution of model's latent parameters. These posterior distributions can be used to make predictions, estimate uncertainty, and evaluate hypotheses.\n",
    "\n",
    "The model assumes that the response variable $y$ is generated from a normal distribution with a mean that is a linear function of the predictors and a constant variance. Mathematically, this can be expressed as:\n",
    "\n",
    "$$\n",
    "y = X\\beta + \\epsilon, \\quad \\epsilon \\sim N(0, \\sigma^2I_n)\n",
    "$$\n",
    "\n",
    "where $y$ is the vector of response variables, $X$ is the design matrix of predictors, $\\beta$ is the vector of regression coefficients, and $\\epsilon$ is the error term with a normal distribution. \n",
    "\n",
    "The likelihood of the data given the parameters is then:\n",
    "\n",
    "$$\n",
    "p(y|X, \\beta, \\sigma^2) = (2\\pi\\sigma^2)^{-n/2} \\exp \\left( -\\frac{1}{2\\sigma^2} (y - X\\beta)'(y - X\\beta) \\right)\n",
    "$$\n",
    "\n",
    "To perform Bayesian inference, we specify prior distributions for the parameters, which include the regression coefficients $\\beta$ and the error variance $\\sigma^2$. \n",
    "\n",
    "$$\n",
    "\\beta \\sim p(\\beta), \\quad \\sigma \\sim p(\\sigma)\n",
    "$$\n",
    "\n",
    "The posterior distribution, which combines the prior information with the likelihood of the observed data, is then derived using Bayes' theorem. The joint posterior distribution of $\\beta$ and $\\sigma$ is:\n",
    "\n",
    "$$\n",
    "p(\\beta, \\sigma | y, X) \\propto p(y | X, \\beta, \\sigma^2) p(\\beta) p(\\sigma)\n",
    "$$\n",
    "\n",
    "Note that the priors are generally assumed to be independent of one another.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "import arviz as az\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pymc as pm\n",
    "import seaborn as sns\n",
    "\n",
    "az.style.use(\"arviz-darkgrid\")\n",
    "RANDOM_SEED = 8296\n",
    "np.random.seed(RANDOM_SEED)\n",
    "\n",
    "warnings.filterwarnings(action=\"ignore\", category=UserWarning, module=r\"seaborn\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "## Example: Fish Weight Prediction\n",
    "\n",
    "In this lesson, we'll imagine we are working in the data science team of an e-commerce company. In particular, we sell really good and fresh fish to our clients (mainly fancy restaurants). \n",
    "\n",
    "When we ship our products, there is a very important piece of information we need: the weight of the fish. This is important for two reasons: \n",
    "\n",
    "1. Because we _bill_ our clients according to weight. \n",
    "\n",
    "2. Because the company that delivers the fish to our clients has different price tiers for weights, and those tiers can get _really_ expensive. So we want to know the probability of an item being above that line. In other words, estimating uncertainty is important here!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "![](images/weighingfish.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "\n",
    "The problem we face is that we purchase our fish in bulk. This means we only know the total weight of our entire order, but we don't have the weights of the individual fish. You might think the obvious solution is simply to weigh each fish one by one.\n",
    "\n",
    "However, this approach has significant drawbacks. Manually weighing each fish is costly, requires a lot of time, and demands substantial labor. This process is inefficient and impractical for our needs.\n",
    "\n",
    "Given these challenges, we need to explore alternative solutions. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### A solution\n",
    "\n",
    "While researching the problem, we discovered that our wholesale supplier has detailed information on the size of each individual fish, including their length, height, and width. Since it is infeasible to weigh individual fish, the supplier uses a **camera** to record the size of each fish. \n",
    "\n",
    "However, the company used to try to weigh each fish manually until costs became prohibitive. As a result, we have a valuable **training dataset** consisting of different types of fish with their accurately -measured weights."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "![](images/fishvideo.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Exploratory data analysis\n",
    "\n",
    "Let's import the data and take a look at it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "fish_market = pd.read_csv(\"../data/fish-market.csv\")\n",
    "fish_market.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "We have collected 159 measurements, and all columns in our dataset have the appropriate data types.\n",
    "\n",
    "For each observation, the dataset includes the following information: the species of the fish, its weight, height, and width, as well as three distinct length measurements. You might be wondering why we have three different measurements for the fish's length. Let's delve into some summary statistics to better understand the data and its significance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "fish_market.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "No missing values, which is nice.\n",
    "\n",
    "Next let's peek at some summary statistics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "fish_market.describe().round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Things to note:\n",
    "\n",
    "- Though there are no missing data, there are some zero-weight fish! -- either the fish was below the minimum weight for the scake, or there was a mistake during data collection. \n",
    "- The standard deviation of the columns are very high, especially for weights.\n",
    "- There are three columns for length, which is interesting. We will explore this further."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "sns.heatmap(\n",
    "    fish_market.drop(columns=\"Species\").corr(),\n",
    "    vmin=-1,\n",
    "    vmax=1,\n",
    "    center=0,\n",
    "    annot=True,\n",
    "    linewidths=4,\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "The three length measurements are highly correlated with each other. This means they essentially carry the same information. Without additional details to distinguish among them, we should arbitrarily choose one measurement and discard the other two. Keeping all three would be redundant and unnecessary since they do not provide unique information.\n",
    "\n",
    "There is nothing inherently Bayesian about this step. The concept of *multicollinearity* is a fundamental concern in both Bayesian and frequentist statistics. In essence, if you include multiple variables that convey similar information in your regression model, you will end up with very unstable parameter estimates. This redundancy does not improve your model's predictive power and can, in fact, lead to misleading results. Thus, it is crucial to identify and address multicollinearity to maintain the robustness and reliability of your model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "fish_market = fish_market.drop([\"Length2\", \"Length3\"], axis=\"columns\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Visual data exploration\n",
    "\n",
    "Its always a good idea to plot your data! Seaborn's `pairplot` function is a great way to visualize the relationships between variables in your dataset. This function creates a matrix of scatterplots, with each variable plotted against every other variable. The diagonal of the matrix shows a histogram of each variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "sns.pairplot(data=fish_market);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "All variables exhibit linear relationships with each other, with one notable exception: weight. Weight appears to increase exponentially in relation to the other variables. However, this exponential growth is not limitless; it plateaus due to a natural upper limit on weight.\n",
    "\n",
    "Additionally, we observe several trends within the data that may indicate differences in how these variables interact across various species. These trends suggest that the relationships between the variables are not uniform across all species, potentially due to unique biological or ecological factors influencing each species.\n",
    "\n",
    "So, let's break down the data by species and see if we can identify any patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "sns.pairplot(data=fish_market, hue=\"Species\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Thus, it is clear that any model we build must account for the differences in the relationships between variables across species. This is where Bayesian linear regression comes in handy. By incorporating **domain knowledge** about the relationships between variables and the differences across species, we can build a more robust and reliable model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 2, figsize=(14, 9))\n",
    "for ax, var in zip(axes.ravel(), [\"Length1\", \"Height\", \"Width\", \"Weight\"]):\n",
    "    sns.violinplot(x=\"Species\", y=var, data=fish_market, ax=ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "The most diverse species are Bream, Whitefish, Perch, and Pike. This diversity likely makes them more versatile for sale and cooking because they come in a wide range of sizes, including different weights, widths, and heights. This variety allows for more options in preparation methods and recipes, catering to various culinary needs.\n",
    "\n",
    "On the other hand, the Smelt is a very small fish that is typically used in specialized recipes. A brief internet search reveals that Smelt is most commonly fried and served as an appetizer, particularly in European cuisine. Its smaller size and specific preparation methods make it less versatile than the more diverse species like Bream, Whitefish, Perch, and Pike.The most diverse species are Bream, Whitefish, Perch and Pike, which means they are probably easier to sell and cook, as they exist in a variety of weight, width and height. Conversely, the Smelt is a very small fish that is probably used only in specialized recipes -- a quick internet search will show you that they are usually fried and served as appetizers, at least in Europe."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## A non-Bayesian linear regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Now that we have a clearer understanding of the data we're working with, let's move on to developing a predictive model. Our specific task is to **predict the weight of a fish based on its width, height, and length**. While we've chosen these particular variables for our analysis, it's important to note that different combinations of independent and dependent variables could also be used, depending on the specific requirements of the study.\n",
    "\n",
    "The most promising approach for our task is to develop a **physical model**. This involves leveraging the inherent relationships between height, width, and weight, which are governed by physical proportions that impose natural lower and upper bounds on these variables. In a professional context, such a model would likely yield the most accurate and reliable predictions due to its basis in the physical characteristics of fish.\n",
    "\n",
    "However, creating a detailed physical model can be quite complex. Therefore, for our initial attempt, we can use a simple **ordinary least squares (OLS)** regression to establish a relationship between the dependent variable (weight) and the independent variables (width, height, and length).\n",
    "\n",
    "From our data exploration, we observed that weight is not linearly related to the other variables. This non-linear relationship suggests that a direct application of linear regression may not be effective. To address this issue, we often need to apply some form of data transformation to better fit the model to the data.\n",
    "\n",
    "In this scenario, a **logarithmic transformation** of the data appears to be a suitable choice. This transformation can help counteract the exponential increase in weight as the fish's width, height, and length increase. By applying a log-transform, we can linearize the relationship between these variables, making it more appropriate for linear regression analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Taking the log of all covariates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "fish_market = fish_market.assign(log_width=np.log(fish_market.Width),\n",
    "                                 log_height=np.log(fish_market.Height),\n",
    "                                 log_length=np.log(fish_market.Length1),\n",
    "                                 log_weight=np.log(fish_market.Weight))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Note that during the transformation, Pandas generated a warning because the logarithm of zero is negative infinity. This warning indicates a problem we need to address.\n",
    "\n",
    "The simplest solution is to remove these observations from our dataset, a process known as **complete case analysis**.\n",
    "\n",
    "This approach makes an implicit assumption that the missing values are not systematically different from the non-missing values. This assumption is often incorrect. For instance, consider that certain characteristics of the missing fish might make them particularly difficult to find in fish markets. If these characteristics contradict the trends observed in the available data, our analysis could become arbitrarily biased.\n",
    "\n",
    "Or, imagine that a specific size of fish makes them ideal prey for predators. Consequently, these fish experience higher stress levels, resulting in them being thinner and lighter than their counterparts. However, their body shape makes them more challenging to weigh accurately, leading to frequent missing measurements.\n",
    "\n",
    "By simply discarding these particular measurements, we might be ignoring a subset of fish that could significantly impact our trendline. This situation suggests that we might need a more sophisticated model to account for the dip in weight at certain size ranges, rather than merely excluding the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Simple OLS regression\n",
    "\n",
    "An easy way to perform OLS regression is via the `seaborn` graphics library. The `lmplot` function creates a scatterplot of the data and fits a regression line to the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "fish_complete = fish_market[fish_market[\"Weight\"] != 0].copy()\n",
    "\n",
    "sns.lmplot(\n",
    "    data=fish_complete,\n",
    "    x=\"log_height\",\n",
    "    y=\"log_weight\",\n",
    "    hue=\"Species\",\n",
    "    col=\"Species\",\n",
    "    height=3,\n",
    "    col_wrap=4,\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "The output here is purely visual, but in log space, our input variables seem linearly related to weight, so there is good reason to believe that a linear model is appropriate here.\n",
    "\n",
    "Let's go ahead and fit a linear model to the data using PyMC."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "### Baseline Model\n",
    "\n",
    "Let's start with a very simple \"null\" model: just a global mean with no predictors. \n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\log(\\text{weight}) &\\sim \\mathrm{Normal}(\\mu, \\sigma)\\\\\n",
    "\\mu &\\sim \\mathrm{Normal}(0, 1)\\\\\n",
    "\\sigma &\\sim \\mathrm{HalfNormal}(1)\\\\\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "This corresponds to `log(weight) ~ 1` in [Wilkinson notation](https://uk.mathworks.com/help/stats/wilkinson-notation.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "with pm.Model() as fish_simple:\n",
    "\n",
    "    # Prior\n",
    "    mu = pm.Normal(\"mu\")\n",
    "    sigma = pm.HalfNormal(\"sigma\", 1.0)\n",
    "\n",
    "    # Likelihood\n",
    "    pm.Normal(\n",
    "        \"log_weight\",\n",
    "        mu=mu,\n",
    "        sigma=sigma,\n",
    "        observed=fish_complete[\"log_weight\"].to_numpy(),\n",
    "    )\n",
    "\n",
    "pm.model_to_graphviz(fish_simple)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Now to fit the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "with fish_simple:\n",
    "    trace_simple = pm.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "az.summary(trace_simple, round_to=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "We will dig into model checking and diagnostics in a later section (which will explain most of the values in the `summary` table), but for now we can plot the posterior distribution of the model parameters and do some informal, visual checks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "az.plot_trace(trace_simple);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Traceplots are useful for evaluating the performance of our MCMC sampling. In these plots, we aim to see a \"fuzzy caterpillar\" pattern on the right side, which indicates that the chains are **mixing well** and exploring the parameter space effectively. This is evidence to suggest the chains have converged (to something!) and are providing a reasonable representation of the posterior distribution.\n",
    "\n",
    "In addition to traceplots, **rank plots** serve as another diagnostic tool for assessing the quality of your MCMC samples. Rank plots display the ranks of sampled values for each parameter, and we look for the histograms in these plots to be approximately uniform. If one chain samples some values significantly more than the other chains, then the ranks of its samples will be markedly higher or lower than other chains and the histograms won't be uniform. Uniform histograms suggest that the sampler has performed well, meaning the samples are not exhibiting degeneracies. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "az.plot_rank(trace_simple);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Interpreting parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "This model is very simple, so the mean coefficient estimates are the mean and standard deviation, respectively, of the sample. \n",
    "\n",
    "If we go back to our trace plot, our posterior uncertainty doesn't seem big. But remember that we're on log scale, so it would be best to work on the nominal scale. Fortunately, `plot_trace` even has a `transform` argument we can use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "az.plot_trace(trace_simple, transform=np.exp, var_names=\"mu\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "So there is a reasonable amount of uncertainty in our estimate of the mean weight of a fish, which is perhaps surprising given we have pooled all the data. \n",
    "\n",
    "Now let's look at **model fit**. We will explore model checking in detail later in the course, but for now, we can use a simple technique: posterior predictive checks.\n",
    "\n",
    "Posterior predictive checks (PPCs) are a great way to validate a model. The idea is to generate data from the model using parameters from the posterior distribution and compare these samples to the observed data.\n",
    "\n",
    "Let's generate these simulated datasets now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "with fish_simple:\n",
    "    ppc = pm.sample_posterior_predictive(trace_simple, extend_inferencedata=True)\n",
    "\n",
    "ax = az.plot_ppc(trace_simple)\n",
    "ax.set_xlabel(\"log_weight\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "The data are clearly heterogeneous, as evidenced by the multiple peaks in the log-weight, but our model fails to capture them accurately. This discrepancy suggests that the model is struggling to fit the data properly. Consequently, the model resorts to increasing the posterior uncertainty and observational noise (`sigma`). Essentially, the model compensates for its inability to accurately represent the data by broadening its predictions, sacrificing precision for coverage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Adding predictors to our model\n",
    "\n",
    "It is time to introduce predictors to our model, and see how much they improve prediction.\n",
    "\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\text{priors}\\\\\n",
    "\\mu[s] &\\sim \\mathrm{Normal}(0, 1)\\\\\n",
    "\\beta[s, k] &\\sim \\mathrm{Normal}(0, 0.5)\\\\\n",
    "\\sigma &\\sim \\mathrm{HalfNormal}(1)\\\\\n",
    "\\text{linear model}\\\\\n",
    "\\mu_i &= \\mu[s_i]\\\\\n",
    "        & \\quad + \\beta[s_i, 0] \\times \\log(\\text{width}_i)\\\\\n",
    "        & \\quad + \\beta[s_i, 1] \\times \\log(\\text{height}_i)\\\\\n",
    "        & \\quad + \\beta[s_i, 2] \\times \\log(\\text{length}_i)\\\\\n",
    "\\text{likelihood}\\\\\n",
    "\\log(\\text{weight}_i) &\\sim \\mathrm{Normal}(\\mu_i, \\sigma)\\\\\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "\n",
    "where $s_i$ is the species index corresponding to fish _i_:\n",
    "\n",
    "\n",
    "$$\n",
    "s_i \\in \\{ 0, 1, \\ldots, {S-1} \\}.\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "In Wilkinson notation, the model can be written as:\n",
    "\n",
    "\n",
    "`log(weight) ~ 0 + species + log(width):species + log(height):species + log(length):species`. \n",
    "\n",
    "\n",
    "The `0 + species` component means that we just have $S$ intercept terms, one for each species, with no global intercept. \n",
    "\n",
    "The remaining terms (e.g. `log(width):species`) represent an interaction between the predictor and the `species` category. So there will be one coefficient for the $\\log(width)$ slope (in this case) for each species."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "So, each species has its own intercept and slopes for width, height, and length. This is an **unpooled model** because we are essentially fitting a separate regression for each species!\n",
    "\n",
    "In order to make this work, we need to encode the species as a categorical variable. We can do this using the `factorize` function in `pandas`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "fish_complete.Species.factorize(sort=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "`factorize` encodes the species names into integer values that we will use as indices for the model parametes. In addition, we will use the category classes as dimension labels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Define dimensions & coordinates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "species_idx, species = fish_complete.Species.factorize(sort=True)\n",
    "coords = {\n",
    "    \"slopes\": [\"width_effect\", \"height_effect\", \"length_effect\"],\n",
    "    \"species\": species,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "We will also make use of `Data` containers to include the data explicitly in the model. This will be useful later, when we want to predict out-of-sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "with pm.Model(coords=coords) as fish_unpooled:\n",
    "    # data\n",
    "    log_width = pm.Data(\"log_width\", fish_complete.log_width.to_numpy())\n",
    "    log_height = pm.Data(\"log_height\", fish_complete.log_height.to_numpy())\n",
    "    log_length = pm.Data(\"log_length\", fish_complete.log_length.to_numpy())\n",
    "    log_weight = pm.Data(\"log_weight\", fish_complete.log_weight.to_numpy())\n",
    "    s = pm.Data(\"species_idx\", species_idx)\n",
    "\n",
    "    # priors\n",
    "    mu = pm.Normal(\"mu\", sigma=1.0, dims=\"species\")\n",
    "\n",
    "    # each species gets a slope for each predictor thx to `dims`:\n",
    "    beta = pm.Normal(\"beta\", sigma=0.5, dims=(\"species\", \"slopes\"))\n",
    "\n",
    "    # linear regression\n",
    "    expected_weight = (\n",
    "        mu[s]\n",
    "        + beta[s, 0] * log_width\n",
    "        + beta[s, 1] * log_height\n",
    "        + beta[s, 2] * log_length\n",
    "    )\n",
    "    # observational noise\n",
    "    sigma = pm.HalfNormal(\"sigma\", 1.0)\n",
    "\n",
    "    # likelihood\n",
    "    pm.Normal(\n",
    "        \"log_obs\",\n",
    "        mu=expected_weight,\n",
    "        sigma=sigma,\n",
    "        observed=log_weight,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "It's always helpful to plot the model before fitting it. This can help you catch errors in the model specification, and also give you a sense of what the model is doing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "pm.model_to_graphviz(fish_unpooled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "with fish_unpooled:\n",
    "    trace_unpooled = pm.sample()\n",
    "    # Posterior predictive\n",
    "    pm.sample_posterior_predictive(trace_unpooled, extend_inferencedata=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Inspecting the posterior paramter estimates, notably the intercepts to begin with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "az.plot_trace(trace_unpooled, var_names='mu', transform=np.exp);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The intercepts look small (even on the nominal scale) which seems odd. But recall how intercepts are interpreted: they are the expected value of the outcome when all predictors are zero. In this case, that means when the log of the width, height, and length are zero. This is an awkward from an interpretive standpoint. \n",
    "\n",
    "How could we improve this?\n",
    "\n",
    "Give it a try, and re-run the improved model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have meaningful intercepts -- the expected weight of a fish with average width, height, and length for each species."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "az.plot_trace(trace_unpooled, var_names='mu', transform=np.exp);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we have vector-valued paramters a forest plot is convenient for visualizing them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "az.plot_forest(trace_unpooled, var_names=\"beta\", transform=np.exp);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "az.plot_trace(trace_unpooled, var_names=\"sigma\", transform=np.exp);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "There is a good sign here: the posterior uncertainty around `sigma` is much lower than before, i.e we picked up much more information on the fish weights. But did this improve our posterior predictions?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "with fish_unpooled:\n",
    "    pm.sample_posterior_predictive(trace_unpooled, extend_inferencedata=True)\n",
    "ax = az.plot_ppc(trace_unpooled)\n",
    "ax.set_xlabel(\"log_obs\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Predicting out-of-sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "In statistical workflows, a common task is to make predictions using new, unseen data, often referred to as \"out-of-sample\" data. In PyMC, the most straightforward approach to achieve this is by utilizing the `Data` container. This container allows PyMC and ArviZ to specify the data used for training the model, and then allow you to modify it later on.\n",
    "\n",
    "#### Splitting Data into Training and Test Sets\n",
    "\n",
    "To illustrate this functionality, let's randomly select 90% of our data as the training dataset for the model, while reserving the remaining 10% as the test data. This test data will be unseen by the model during the training process, allowing us to evaluate its performance on new, previously unseen data when making predictions.\n",
    "\n",
    "By following this approach, you can effectively train your model on a subset of the available data and then assess its predictive capabilities on the held-out test data, mimicking real-world scenarios where predictions need to be made on new, unobserved data points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "fish_test = (\n",
    "    fish_complete.sample(frac=0.1, random_state=1)\n",
    "    .sort_index()\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "test_idx = fish_test.index\n",
    "fish_train = fish_complete.loc[fish_complete.index.difference(test_idx)].reset_index(\n",
    "    drop=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Since the dataset changed compared to the previous model, we also have to redefine our coordinates:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "species_idx, species = fish_train.Species.factorize(sort=True)\n",
    "coords[\"species\"] = species"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "with pm.Model(\n",
    "    coords=coords, coords_mutable={\"obs_idx\": fish_train.index}\n",
    ") as fish_unpooled_oos:\n",
    "    # data\n",
    "    log_width = pm.Data(\n",
    "        \"log_width\", fish_train.log_width.to_numpy() - fish_train.log_width.mean(), dims=\"obs_idx\"\n",
    "    )\n",
    "    log_height = pm.Data(\n",
    "        \"log_height\", fish_train.log_height.to_numpy() - fish_train.log_height.mean(), dims=\"obs_idx\"\n",
    "    )\n",
    "    log_length = pm.Data(\n",
    "        \"log_length\", fish_train.log_length.to_numpy() - fish_train.log_length.mean(), dims=\"obs_idx\"\n",
    "    )\n",
    "    log_weight = pm.Data(\n",
    "        \"log_weight\", fish_train.log_weight.to_numpy(), dims=\"obs_idx\"\n",
    "    )\n",
    "    species_idx_ = pm.Data(\"species_idx\", species_idx, dims=\"obs_idx\")\n",
    "\n",
    "    # priors\n",
    "    mu = pm.Normal(\"mu\", sigma=1.0, dims=\"species\")\n",
    "    beta = pm.Normal(\"beta\", sigma=0.5, dims=(\"slopes\", \"species\"))\n",
    "\n",
    "    # linear regression\n",
    "    expected_weight = (\n",
    "        mu[species_idx_]\n",
    "        + beta[0, species_idx_] * log_width\n",
    "        + beta[1, species_idx_] * log_height\n",
    "        + beta[2, species_idx_] * log_length\n",
    "    )\n",
    "    # observational noise\n",
    "    sigma = pm.HalfNormal(\"sigma\", 1.0)\n",
    "\n",
    "    # likelihood\n",
    "    log_obs = pm.Normal(\n",
    "        \"log_obs\", mu=expected_weight, sigma=sigma, observed=log_weight, dims=\"obs_idx\"\n",
    "    )\n",
    "\n",
    "    # sampling\n",
    "    trace_unpooled_oos = pm.sample()\n",
    "    pm.sample_posterior_predictive(trace_unpooled_oos, extend_inferencedata=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "pm.model_to_graphviz(fish_unpooled_oos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Checking the traceplots:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "az.plot_trace(trace_unpooled_oos, transform=np.exp);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Now we want to see how this model would work in production: given some fish morphometrics, can we accurately predict the weight of the fish?\n",
    "\n",
    "To do this, we just have to use `set_data` to change the inputs from the training set to the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Encode the species\n",
    "species_idx_test = pd.Categorical(fish_test.Species, categories=species).codes.astype(\n",
    "    np.int64\n",
    ")\n",
    "\n",
    "species_idx_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Note that we are shifting the input variables using the training set mean and standard deviation. You always want to use the same transformation on the test set as you did on the training set!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "with fish_unpooled_oos:\n",
    "    pm.set_data(\n",
    "        coords={\"obs_idx\": fish_test.index},\n",
    "        new_data={\n",
    "            \"log_height\": fish_test.log_height.to_numpy() - fish_train.log_height.mean(),\n",
    "            \"log_length\": fish_test.log_length.to_numpy() - fish_train.log_length.mean(),\n",
    "            \"log_width\": fish_test.log_width.to_numpy() - fish_train.log_width.mean(),\n",
    "            \"log_weight\": np.zeros_like(fish_test.index),\n",
    "            \"species_idx\": species_idx_test,\n",
    "        },\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "We now call `sample_posterior_predictive` once again, but this time we specify `predictions=True` since these are not posterior predictive checks, and they will be store in a different attribute on the trace."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Use updated values to predict outcomes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "with fish_unpooled_oos:\n",
    "    pm.sample_posterior_predictive(\n",
    "        trace_unpooled_oos,\n",
    "        predictions=True,\n",
    "        extend_inferencedata=True,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "trace_unpooled_oos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "How good are these imputations? Glad you asked. Remember that our data are not _really_ out-of-sample; we just cut them out from our original dataset, so we can compare our predictions to the true weights. This is a simple line of code in ArviZ (we just exponentiate the predicted log weights to compare them to the true weights):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "az.plot_posterior(\n",
    "    trace_unpooled_oos.predictions,\n",
    "    ref_val=fish_test[\"Weight\"].tolist(),\n",
    "    transform=np.exp,\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "So the predicted values all fell within the predictive distributions -- not all within the 95% interval, but there were no extreme predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise: Refitting the model\n",
    "\n",
    "Given the success of the model, you go back and try to fit it to data collected by another vendor, only to find that the predictions aren't nearly as good!\n",
    "\n",
    "Frustrated, you go back to the drawing board... they deal with the same type of fish, but what's wrong with their data?\n",
    "\n",
    "One of their colleagues mentions something about not having use the same equipment to weight the fish, because the \"old manager always tried to cut costs\".\n",
    "They used a much cheaper scale ...\n",
    "\n",
    "With this information in hand, make the appropriate modifications to the model to accomodate the new data. \n",
    "\n",
    "Here is the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_fish = pd.read_csv(\"../data/new_fish.csv\", index_col=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try to diagnose the issue and propose a new model (a slight variation) that may help in dealing with the properties of this new dataset better!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## From predictions to business insights\n",
    "\n",
    "Recall from the introduction that there are different price tiers for weights, and those tiers can get _really_ expensive, so we want to know the probability of an item being above any theshold.\n",
    "\n",
    "- $> 250$\n",
    "- $> 500$\n",
    "- $> 750$\n",
    "- $> 1000$\n",
    "\n",
    "Since we have calculated posterior distributions, we have the ability to compute these probabilities for any new fish we observe.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# Extract projections to numpy array\n",
    "predictions = (\n",
    "    np.exp(\n",
    "        az.extract(trace_unpooled_oos.predictions)\n",
    "        .to_array()\n",
    "        .to_numpy()\n",
    "        .squeeze()\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can see what proportion are above $250$ grams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "-"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "threshold = 250\n",
    "(predictions >= threshold).mean(axis=1).round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "If we take something like a 0.5 probability as being \"above\", we can make a decision about each:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "(predictions >= threshold).mean(axis=1).round(2) > 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "\n",
    "But remember that there are four thresholds $(250, 500, 750, 1000)$, so let's generalize this approach to the other three thresholds. We'll also plot these probabilities of being above thresholds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "predictions = np.exp(trace_unpooled_oos.predictions)\n",
    "\n",
    "axes = az.plot_posterior(predictions, color=\"k\")\n",
    "\n",
    "for k, threshold in enumerate([250, 500, 750, 1000]):\n",
    "    probs_above_threshold = (predictions >= threshold).mean(dim=(\"chain\", \"draw\"))\n",
    "\n",
    "    for i, ax in enumerate(axes.ravel()):\n",
    "        ax.axvline(threshold, color=f\"C{k}\")\n",
    "        _, pdf = az.kde(\n",
    "            predictions[\"log_obs\"].sel(obs_idx=i).stack(sample=(\"chain\", \"draw\")).data\n",
    "        )\n",
    "        ax.text(\n",
    "            x=threshold - 35,\n",
    "            y=pdf.max() / 2,\n",
    "            s=f\">={threshold}\",\n",
    "            color=f\"C{k}\",\n",
    "            fontsize=\"16\",\n",
    "            fontweight=\"bold\",\n",
    "        )\n",
    "        ax.text(\n",
    "            x=threshold - 20,\n",
    "            y=pdf.max() / 2.3,\n",
    "            s=f\"{probs_above_threshold.sel(obs_idx=i)['log_obs'].data * 100:.0f}%\",\n",
    "            color=f\"C{k}\",\n",
    "            fontsize=\"16\",\n",
    "            fontweight=\"bold\",\n",
    "        )\n",
    "        ax.set_title(f\"New fish\\n{i}\", fontsize=16)\n",
    "        ax.set(xlabel=\"Weight\\n\", ylabel=\"Plausible values\")\n",
    "plt.suptitle(\n",
    "    \"Probability of weighing more than thresholds\", fontsize=26, fontweight=\"bold\"\n",
    ");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext watermark\n",
    "%watermark -n -u -v -iv -w"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "hide_code_all_hidden": false,
  "kernelspec": {
   "display_name": "bayes_course",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
