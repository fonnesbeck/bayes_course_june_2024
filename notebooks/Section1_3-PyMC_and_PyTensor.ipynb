{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/fonnesbeck/bayes_course_dec_2023/blob/master/notebooks/Section1_3-Pymc_and_Pytensor.ipynb)\n",
    "\n",
    "# PyMC and PyTensor\n",
    "\n",
    "Before we dive into building Bayesian models in PyMC, we want to give a brief introduction to the PyTensor library. PyTensor is a Python library that allows you to define, optimize/rewrite, and evaluate mathematical expressions involving multi-dimensional arrays efficiently. It serves as the computational backend of PyMC.\n",
    "\n",
    "![pytensor logo](images/PyTensor_RGB.png)\n",
    "\n",
    "Some of PyTensorâ€™s features are:\n",
    "\n",
    "- Tight integration with NumPy - Use `numpy.ndarray` in PyTensor-compiled functions\n",
    "- Efficient symbolic differentiation - PyTensor efficiently computes your derivatives for functions with one or many inputs\n",
    "- Speed and stability optimizations - Get the right answer for log(1 + x) even when x is near zero\n",
    "- Dynamic C/JAX/Numba code generation - Evaluate expressions faster\n",
    "\n",
    "PyTensor is based on Theano, which has been powering large-scale computationally intensive scientific investigations since 2007.\n",
    "\n",
    "In this notebook we want to give an introduction of how PyMC models translate to PyTensor graphs. The purpose is not to give a detailed description of all [`pytensor`](https://github.com/pytensor-devs/pytensor)'s capabilities but rather focus on the main concepts to understand its connection with PyMC. For a more detailed description of the project please refer to the official documentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import pytensor\n",
    "import pytensor.tensor as pt\n",
    "import pymc as pm\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import scipy.stats\n",
    "\n",
    "RANDOM_SEED = 20090425"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A simple example\n",
    "\n",
    "To begin, we define some pytensor tensors and show how to perform some basic operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = pt.scalar(name=\"x\")\n",
    "y = pt.vector(name=\"y\")\n",
    "\n",
    "print(\n",
    "    f\"\"\"\n",
    "x type: {x.type}\n",
    "x name = {x.name}\n",
    "---\n",
    "y type: {y.type}\n",
    "y name = {y.name}\n",
    "\"\"\"\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have defined the `x` and `y` tensors, we can create a new one by adding them together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = x + y\n",
    "z.name = \"x + y\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make the computation a bit more complex let us take the logarithm of the resulting tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = pt.log(z)\n",
    "w.name = \"log(x + y)\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the `pytensor.dprint` function to print the computational graph of any given tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pytensor.dprint(w)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that this graph does not do any computation (yet!). It is simply defining the sequence of steps to be done. We can use `pytensor.function` to define a callable object so that we can push values trough the graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = pytensor.function(inputs=[x, y], outputs=w)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the graph is compiled, we can push some concrete values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f(x=0, y=[1, np.e])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> TIP:\n",
    "> Sometimes we just want to debug, we can use `pytensor.graph.basic.Variable.eval` for that:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w.eval({x: 0, y: [1, np.e]})"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can set intermediate values as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w.eval({z: [1, np.e]})"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PyTensor is clever!\n",
    "\n",
    "One of the most important features of `pytensor` is that it can automatically optimize the mathematical operations inside a graph. Let's consider a simple example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = pt.scalar(name=\"a\")\n",
    "b = pt.scalar(name=\"b\")\n",
    "\n",
    "c = a / b\n",
    "c.name = \"a / b\"\n",
    "\n",
    "pytensor.dprint(c)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let us multiply `b` times `c`. This should result in simply `a`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = b * c\n",
    "d.name = \"b * c\"\n",
    "\n",
    "pytensor.dprint(d)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The graph shows the full computation, but once we compile it the operation becomes the identity on `a` as expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = pytensor.function(inputs=[a, b], outputs=d)\n",
    "\n",
    "pytensor.dprint(g)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is in an PyTensor graph?\n",
    "\n",
    "The following diagram shows the basic structure of an `pytensor` graph.\n",
    "\n",
    "![pytensor graph](images/apply.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can can make these concepts more tangible by explicitly indicating them in the first example from the section above. Let us compute the graph components for the tensor `z`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    f\"\"\"\n",
    "z type: {z.type}\n",
    "z name = {z.name}\n",
    "z owner = {z.owner}\n",
    "z owner inputs = {z.owner.inputs}\n",
    "z owner op = {z.owner.op}\n",
    "z owner output = {z.owner.outputs}\n",
    "\"\"\"\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code snippet helps us understand these concepts by going through the computational graph of `w`. The actual code is not as important here, the focus is on the outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start from the top\n",
    "stack = [w]\n",
    "\n",
    "while stack:\n",
    "    print(\"---\")\n",
    "    var = stack.pop(0)\n",
    "    print(f\"Checking variable {var} of type {var.type}\")\n",
    "    # check variable is not a root variable\n",
    "    if var.owner is not None:\n",
    "        print(f\" > Op is {var.owner.op}\")\n",
    "        # loop over the inputs\n",
    "        for i, input in enumerate(var.owner.inputs):\n",
    "            print(f\" > Input {i} is {input}\")\n",
    "            stack.append(input)\n",
    "    else:\n",
    "        print(f\" > {var} is a root variable\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that this is very similar to the output of `pytensor.dprint` function introduced above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pytensor.dprint(w)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: Create and manipulate PyTensor objects\n",
    "\n",
    "To give you some practice with basic PyTensor data structures and functions, try making the operations below work by implementing the functions that are needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def make_vector():\n",
    "    \"\"\"\n",
    "    Create and return a new Aesara vector.\n",
    "    \"\"\"\n",
    "\n",
    "    pass\n",
    "\n",
    "def make_matrix():\n",
    "    \"\"\"\n",
    "    Create and return a new Aesara matrix.\n",
    "    \"\"\"\n",
    "\n",
    "    pass\n",
    "\n",
    "def elemwise_mul(a, b):\n",
    "    \"\"\"\n",
    "    a: An PyTensor matrix\n",
    "    b: An PyTensor matrix\n",
    "    \n",
    "    Calcuate the elementwise product of a and b and return it\n",
    "    \"\"\"\n",
    "\n",
    "    pass\n",
    "\n",
    "def matrix_vector_mul(a, b):\n",
    "    \"\"\"\n",
    "    a: An PyTensor matrix\n",
    "    b: An PyTensor vector\n",
    "    \n",
    "    Calculate the matrix-vector product of a and b and return it\n",
    "    \"\"\"\n",
    "\n",
    "    pass\n",
    "\n",
    "a = make_vector()\n",
    "b = make_vector()\n",
    "c = elemwise_mul(a, b)\n",
    "d = make_matrix()\n",
    "e = matrix_vector_mul(d, c)\n",
    "\n",
    "f = pytensor.function([a, b, d], e)\n",
    "\n",
    "rng = np.random.RandomState([1, 2, 3])\n",
    "a_value = rng.randn(5).astype(a.dtype)\n",
    "b_value = rng.rand(5).astype(b.dtype)\n",
    "c_value = a_value * b_value\n",
    "d_value = rng.randn(5, 5).astype(d.dtype)\n",
    "expected = np.dot(d_value, c_value)\n",
    "\n",
    "actual = f(a_value, b_value, d_value)\n",
    "\n",
    "assert np.allclose(actual, expected)\n",
    "print(\"SUCCESS!\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example: Logistic regression\n",
    "\n",
    "Here is a non-trivial example, which uses PyTensor to estimate the parameters of a logistic regression model using gradient information. We will use a bioassay example as a test case.\n",
    "\n",
    "Gelman et al. (2003) present an example of an acute toxicity test, commonly performed on animals to estimate the toxicity of various compounds.\n",
    "\n",
    "In this dataset `log_dose` includes 4 levels of dosage, on the log scale, each administered to 5 rats during the experiment. The response variable is `death`, the number of positive responses to the dosage.\n",
    "\n",
    "The number of deaths can be modeled as a binomial response, with the probability of death being a linear function of dose:\n",
    "\n",
    "$$\\begin{aligned}\n",
    "y_i &\\sim \\text{Bin}(n_i, p_i) \\\\\n",
    "\\text{logit}(p_i) &= a + b x_i\n",
    "\\end{aligned}$$\n",
    "\n",
    "The common statistic of interest in such experiments is the **LD50**, the dosage at which the probability of death is 50%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "rng = np.random\n",
    "\n",
    "dose = np.array([-0.86, -0.3 , -0.05,  0.73])\n",
    "deaths = np.array([0, 1, 3, 5])\n",
    "training_steps = 1000"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's declare our symbolic variables.\n",
    "\n",
    "This code introduces a few new concepts. The `shared` function constructs so-called shared variables. These are hybrid symbolic and non-symbolic variables whose value may be shared between multiple functions. Shared variables can be used in symbolic expressions just like the objects returned by `dmatrices` but they also have an internal value that defines the value taken by this symbolic variable in all the functions that use it. It is called a shared variable because its value is shared between many functions. The value can be accessed and modified by the `get_value()` and `set_value()` methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pt.vector(\"X\")\n",
    "Y = pt.vector(\"Y\")\n",
    "b = pytensor.shared(1., name=\"b\")\n",
    "a = pytensor.shared(0., name=\"a\")\n",
    "\n",
    "print(\"Initial model:\", a.get_value(), b.get_value())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... then construct the expression graph:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Probability that target = 1\n",
    "p_1 = 1 / (1 + pt.exp(-(a +X*b))) \n",
    "\n",
    "# The prediction threshold\n",
    "prediction = p_1 > 0.5         \n",
    "\n",
    "# Cross-entropy loss function\n",
    "xent = -Y * pt.log(p_1) - (5-Y) * pt.log(1-p_1) \n",
    "\n",
    "# The cost to minimize\n",
    "cost = xent.mean()      \n",
    "\n",
    "# Compute the gradient of the cost\n",
    "ga, gb = pt.grad(cost, [a, b])                  "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compile PyTensor functions for training the model and predicting from it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "step = pytensor.shared(10., name='step')\n",
    "train = pytensor.function(\n",
    "          inputs=[X, Y],\n",
    "          outputs=[prediction, xent],\n",
    "          updates=((a, a - step * ga), (b, b - step * gb), (step, step * 0.99)),\n",
    "          mode='NUMBA')\n",
    "predict = pytensor.function(inputs=[X], outputs=prediction, mode='NUMBA')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can train the model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(training_steps):\n",
    "    pred, err = train(dose, deaths)\n",
    "    \n",
    "a_value, b_value = a.get_value(), b.get_value()\n",
    "\n",
    "print(\"Final model:\", a_value, b_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "logit = lambda x: 1. / (1 + np.exp(-x))\n",
    "xvals = np.linspace(-1, 1)\n",
    "plt.plot(xvals, logit(b_value*xvals + a_value))\n",
    "plt.plot(dose, deaths/5., 'ro')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "\n",
    "How do we obtain LD50 (the dose for which there is 50% mortality) from the model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your answer here"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Graph manipulation 101\n",
    "\n",
    "Another interesting feature of PyTensor is the ability to manipulate the computational graph, something that is not possible with TensorFlow or PyTorch. Here we continue with the example above in order to illustrate the main idea around this technique."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get input tensors\n",
    "list(pytensor.graph.graph_inputs(graphs=[w]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a simple example, let's add an `pytensor.tensor.exp` before the `pytensor.tensor.log` (to get the identity function)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parent_of_w = w.owner.inputs[0]  # get z tensor\n",
    "new_parent_of_w = pt.exp(parent_of_w)  # modify the parent of w\n",
    "new_parent_of_w.name = \"exp(x + y)\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the graph of `w` has actually not changed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pytensor.dprint(w)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To modify the graph we need to use the `pytensor.clone_replace` function, which *returns a copy of the initial subgraph with the corresponding substitutions.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_w = pytensor.clone_replace(output=[w], replace={parent_of_w: new_parent_of_w})[0]\n",
    "new_w.name = \"log(exp(x + y))\"\n",
    "pytensor.dprint(new_w)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can test the modified graph by passing some input to the new graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_w.eval({x: 0, y: [1, np.e]})"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, the new graph is just the identity function."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> NOTE:\n",
    "> Again, note that `pytensor` is clever enough to omit the `exp` and `log` once we compile the function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = pytensor.function(inputs=[x, y], outputs=new_w)\n",
    "\n",
    "pytensor.dprint(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f(x=0, y=[1, np.e])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PyTensor RandomVariables\n",
    "\n",
    "Now that we have seen pytensor's basics we want to move in the direction of random variables.\n",
    "\n",
    "How do we generate random numbers in [`numpy`](https://github.com/numpy/numpy)? To illustrate it we can sample from a normal distribution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.random.normal(loc=0, scale=1, size=1_000)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "ax.hist(a, color=\"C0\", bins=15)\n",
    "ax.set(title=\"Samples from a normal distribution using numpy\", ylabel=\"count\");"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's try to do it in PyTensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = pt.random.normal(loc=0, scale=1, name=\"y\")\n",
    "y.type"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we show the graph using `pytensor.dprint`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pytensor.dprint(y)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The inputs are always in the following order:\n",
    "1. `rng` shared variable\n",
    "2. `size`\n",
    "3. `dtype` (number code)\n",
    "4. `arg1`, `arg2` ... `argn`"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We *could* sample by calling `eval()`. on the random variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.eval()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note however that these samples are always the same!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    print(f\"Sample {i}: {y.eval()}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We always get the same samples! This has to do with the random seed step in the graph, i.e. `RandomGeneratorSharedVariable` (we will not go deeper into this subject here). We will show how to generate different samples with `pymc` below."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To do so, we start by defining a `pymc` normal distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = pm.Normal.dist(mu=0, sigma=1)\n",
    "pytensor.dprint(x)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observe that `x` is just a normal `RandomVariable` and which is the same as `y` except for the `rng`."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can try to generate samples by calling `eval()` as above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    print(f\"Sample {i}: {x.eval()}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As before we get the same value for all iterations. The correct way to generate random samples is using `pymc.draw`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "ax.hist(pm.draw(x, draws=1_000), color=\"C1\", bins=15)\n",
    "ax.set(title=\"Samples from a normal distribution using pymc\", ylabel=\"count\");"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yay! We learned how to sample from a `pymc` distribution!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is going on behind the scenes?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now look into how this is done inside a `pymc.Model`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pm.Model() as model:\n",
    "    z = pm.Normal(name=\"z\", mu=np.array([0, 0]), sigma=np.array([1, 2]))\n",
    "\n",
    "pytensor.dprint(z)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are just creating random variables like we saw before, but now registering them in a `pymc` model. To extract the list of random variables we can simply do:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.basic_RVs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pytensor.dprint(model.basic_RVs[0])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can try to sample via `eval()` as above and it is no surprise that we are getting the same samples at each iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    print(f\"Sample {i}: {z.eval()}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, the correct way of sampling is via `pymc.draw`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    print(f\"Sample {i}: {pm.draw(z)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(8, 8))\n",
    "z_draws = pm.draw(vars=z, draws=10_000)\n",
    "ax.hist2d(x=z_draws[:, 0], y=z_draws[:, 1], bins=25)\n",
    "ax.set(title=\"Samples Histogram\");"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Enough with Random Variables, I want to see some (log)probabilities!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall we have defined the following model above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`pymc` is able to convert `RandomVariable`s to their respective probability functions. One simple way is to use `pymc.logp`, which takes as first input a RandomVariable, and as second input the value at which the logp is evaluated (we will discuss this in more detail later)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_value = pt.vector(name=\"z\")\n",
    "z_logp = pm.logp(rv=z, value=z_value)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`z_logp` contains the PyTensor graph that represents the log-probability of the normal random variable `z`, evaluated at `z_value`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pytensor.dprint(z_logp)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> TIP:\n",
    "> There is also a handy `pymc` function to compute the log cumulative probability of a random variable `pymc.logcdf`."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observe that, as explained at the beginning, there has been no computation yet. The actual computation is performed after compiling and passing the input. For illustration purposes alone, we will again use the handy `eval()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_logp.eval({z_value: [0, 0]})"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is nothing other than an evaluation of the log probability of a normal distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scipy.stats.norm.logpdf(x=np.array([0, 0]), loc=np.array([0, 0]), scale=np.array([1, 2]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`pymc` models provide some helpful routines to facilitate the conversion of `RandomVariable`s to probability functions. The model `logp` method, for instance can be used to extract the joint probability of all variables in the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pytensor.dprint(model.logp(sum=False))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because we only have one variable, this function is equivalent to what we obtained by manually calling `pm.logp` before. We can also use a helper method `pymc.Model.compile_logp` to return an already compiled PyTensor function of the model logp."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logp_function = model.compile_logp(sum=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function expects a \"point\" dictionary as input. We could create it ourselves, but just to illustrate another useful `Model` method, let's call `Model.initial_point`, which returns the point that most samplers use when deciding where to start sampling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "point = model.initial_point()\n",
    "point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logp_function(point)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What are value variables and why are they important?\n",
    "\n",
    "As he have seen above, a logp graph does not have random variables. Instead it is defined in terms of input (value) variables. When we want to sample, each random variable (RV) is replaced by a logp function evaluated at the respective input (value) variable. Let's see how this works through some examples. RV and value variables can be observed in these [`scipy`](https://github.com/scipy/scipy) operations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rv = scipy.stats.norm(0, 1)\n",
    "\n",
    "# Equivalent to rv = pm.Normal(\"rv\", 0, 1)\n",
    "rv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Equivalent to rv_draw = pm.draw(rv, 3)\n",
    "rv.rvs(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Equivalent to rv_logp = pm.logp(rv, 1.25)\n",
    "rv.logpdf(1.25)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's look at how these value variables behave in a slightly more complex model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pm.Model() as model_2:\n",
    "    mu = pm.Normal(name=\"mu\", mu=0, sigma=2)\n",
    "    sigma = pm.HalfNormal(name=\"sigma\", sigma=3)\n",
    "    x = pm.Normal(name=\"x\", mu=mu, sigma=sigma)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each model RV is related to a \"value variable\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_2.rvs_to_values"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observe that for sigma the associated value is in the *log* scale as in practice we require unbounded values for efficient Markov chain Monte Carlo (MCMC) sampling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_2.value_vars"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we know how to extract the model variables, we can compute the element-wise log-probability of the model for specific values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract values as pytensor.tensor.var.TensorVariable\n",
    "mu_value = model_2.rvs_to_values[mu]\n",
    "sigma_log_value = model_2.rvs_to_values[sigma]\n",
    "x_value = model_2.rvs_to_values[x]\n",
    "# element-wise log-probability of the model (we do not take te sum)\n",
    "logp_graph = pt.stack(model_2.logp(sum=False))\n",
    "# evaluate by passing concrete values\n",
    "logp_graph.eval({mu_value: 0, sigma_log_value: -10, x_value: 0})"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This equivalent to:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    f\"\"\"\n",
    "mu_value -> {scipy.stats.norm.logpdf(x=0, loc=0, scale=2)}\n",
    "sigma_log_value -> {- 10 + scipy.stats.halfnorm.logpdf(x=np.exp(-10), loc=0, scale=3)} \n",
    "x_value -> {scipy.stats.norm.logpdf(x=0, loc=0, scale=np.exp(-10))}\n",
    "\"\"\"\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> NOTE:\n",
    "> For `sigma_log_value` we add the $-10$ term for the `scipy` and `pytensor` to match because of the jacobian."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we already saw, we can also use the method `compile_logp` to obtain a compiled pytensor function of the model logp, which takes a dictionary of `{value variable name : value}` as inputs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_2.compile_logp(sum=False)({\"mu\": 0, \"sigma_log__\": -10, \"x\": 0})"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `pymc.Model` class also has methods to extract the gradient `dlogp` and the hessian `d2logp` of the logp."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to go deeper into the internals of `pytensor` RandomVariables and `pymc` distributions please take a look into the [distribution developer guide](https://www.pymc.io/projects/docs/en/stable/contributing/implementing_distribution.html)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Debugging"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are various levels on which to debug a model. One of the simplest is to just print out the values that different variables are taking on.\n",
    "\n",
    "Because `PyMC` uses `PyTensor` expressions to build the model rather than functions, there is no way to place a `print` statement into a likelihood function. Instead, you can use the `pytensor.printing.Print` class to print intermediate values."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to print intermediate values of `PyTensor` functions\n",
    "Since `PyTensor` functions are compiled to C (or some other backend), you have to use `pytensor.printing.Print` class to print intermediate values (imported  below as `Print`). Python `print` function will not work. Below is a simple example of using `Print`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = pt.dvector(\"x\")\n",
    "y = pt.dvector(\"y\")\n",
    "func = pytensor.function([x, y], 1 / (x - y))\n",
    "func([1, 2, 3], [1, 0, -1])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To see what causes the `inf` value in the output, we can print intermediate values of $(x-y)$ using `Print`. `Print` class simply passes along its caller but prints out its value along a user-define message:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Print = pytensor.printing.Print\n",
    "\n",
    "z_with_print = Print(\"x - y = \")(x - y)\n",
    "func_with_print = pytensor.function([x, y], 1 / z_with_print)\n",
    "func_with_print([1, 2, 3], [1, 0, -1])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Print` reveals the root cause: $(x-y)$ takes a zero value when $x=1, y=1$, causing the `inf` output."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to capture `Print` output for further analysis\n",
    "\n",
    "When we expect many rows of output from `Print`, it can be desirable to redirect the output to a string buffer and access the values later on. Here is a toy example using Python `print` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "from io import StringIO\n",
    "\n",
    "old_stdout = sys.stdout\n",
    "mystdout = sys.stdout = StringIO()\n",
    "\n",
    "for i in range(5):\n",
    "    print(f\"Test values: {i}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = mystdout.getvalue().split(\"\\n\")\n",
    "sys.stdout = old_stdout  # setting sys.stdout back\n",
    "output"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Troubleshooting a toy PyMC model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = np.random.default_rng(RANDOM_SEED)\n",
    "x = rng.normal(size=100)\n",
    "\n",
    "with pm.Model() as model:\n",
    "    # priors\n",
    "    mu = pm.Normal(\"mu\", mu=0, sigma=1)\n",
    "    sd = pm.Normal(\"sd\", mu=0, sigma=1)\n",
    "\n",
    "    # setting out printing for mu and sd\n",
    "    mu_print = Print(\"mu\")(mu)\n",
    "    sd_print = Print(\"sd\")(sd)\n",
    "\n",
    "    # likelihood\n",
    "    obs = pm.Normal(\"obs\", mu=mu_print, sigma=sd_print, observed=x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pm.model_to_graphviz(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with model:\n",
    "    step = pm.Metropolis()\n",
    "    trace = pm.sample(5, step=step, tune=0, chains=1, progressbar=False, random_seed=RANDOM_SEED)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exception handling of PyMC has improved, so now SamplingError exception prints out the intermediate values of `mu` and `sd` which led to likelihood of `-inf`. However, this technique of printing intermediate values with `pytensor.printing.Print` can be valuable in more complicated cases."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bringing it all together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = np.random.default_rng(RANDOM_SEED)\n",
    "y = rng.normal(loc=5, size=20)\n",
    "\n",
    "old_stdout = sys.stdout\n",
    "mystdout = sys.stdout = StringIO()\n",
    "\n",
    "with pm.Model() as model:\n",
    "    mu = pm.Normal(\"mu\", mu=0, sigma=10)\n",
    "    a = pm.Normal(\"a\", mu=0, sigma=10, initval=0.1)\n",
    "    b = pm.Normal(\"b\", mu=0, sigma=10, initval=0.1)\n",
    "    sd_print = Print(\"Delta\")(a / b)\n",
    "    obs = pm.Normal(\"obs\", mu=mu, sigma=sd_print, observed=y)\n",
    "\n",
    "    # limiting number of samples and chains to simplify output\n",
    "    trace = pm.sample(draws=10, tune=0, chains=1, progressbar=False, random_seed=RANDOM_SEED)\n",
    "\n",
    "output = mystdout.getvalue()\n",
    "sys.stdout = old_stdout  # setting sys.stdout back"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Raw output is a bit messy and requires some cleanup and formatting to convert to `numpy.ndarray`. In the example below regex is used to clean up the output, and then it is evaluated with `eval` to give a list of floats. Code below also works with higher-dimensional outputs (in case you want to experiment with different models)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# output cleanup and conversion to numpy array\n",
    "# this is code accepts more complicated inputs\n",
    "pattern = re.compile(\"Delta __str__ = \")\n",
    "output = re.sub(pattern, \" \", output)\n",
    "pattern = re.compile(\"\\\\s+\")\n",
    "output = re.sub(pattern, \",\", output)\n",
    "pattern = re.compile(r\"\\[,\")\n",
    "output = re.sub(pattern, \"[\", output)\n",
    "output += \"]\"\n",
    "output = \"[\" + output[1:]\n",
    "output = eval(output)\n",
    "output = np.array(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that we requested 5 draws, but got 34 sets of $a/b$ values. The reason is that for each iteration, all proposed values are printed (not just the accepted values). Negative values are clearly problematic."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## References\n",
    "\n",
    "[PyTensor: Getting Started](https://pytensor.readthedocs.io/en/latest/introduction.html)\n",
    "\n",
    "[PyTensor: Implementing a Random Variable Distribution](https://www.pymc.io/projects/docs/en/stable/contributing/implementing_distribution.html)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "dIYxBNT_5HgW",
    "JhmIBByY6T9h",
    "k7spOYvvTdZL",
    "qIMN2gHGzKof",
    "aMM0sJy6z7Ur",
    "WY6bUgqZztC3",
    "58gng2f17_P7",
    "C8Us4nEyhRdu",
    "wkZR0gDWRAgK",
    "wPw9kCvASOeJ",
    "WdZcUfvLUkwK",
    "EHH1hP0uzaWG",
    "I_Ph4o7ZW_XD",
    "jOI-E76fZ2bG",
    "Rq2W2fmobmFg"
   ],
   "name": "PyMC intro.ipynb",
   "provenance": []
  },
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "vscode": {
   "interpreter": {
    "hash": "867ba48c05011db76db56a12fb95ccd32f7ac276df8f4ae698e0d475911a6ba0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
