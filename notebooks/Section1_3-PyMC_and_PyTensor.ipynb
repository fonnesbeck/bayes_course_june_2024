{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/fonnesbeck/bayes_course_june_2024/blob/master/notebooks/Section2-PyMC_Intro.ipynb)\n",
    "\n",
    "# Building Models in PyMC\n",
    "\n",
    "Now that we have been introduced to PyMC at a very high level, let's now take a more detailed look at PyMC's API as it relates to building models.\n",
    "\n",
    "## PyTensor\n",
    "\n",
    "Before we dive into building Bayesian models in PyMC, we want to give a brief introduction to the PyTensor library. PyTensor is a Python library that allows you to define, optimize/rewrite, and evaluate mathematical expressions involving multi-dimensional arrays efficiently. It serves as the computational backend of PyMC.\n",
    "\n",
    "![pytensor logo](images/PyTensor_RGB.png)\n",
    "\n",
    "Some of PyTensor’s features are:\n",
    "\n",
    "- Tight integration with NumPy - Use `numpy.ndarray` in PyTensor-compiled functions\n",
    "- Efficient symbolic differentiation - PyTensor efficiently computes your derivatives for functions with one or many inputs\n",
    "- Speed and stability optimizations - Get the right answer for log(1 + x) even when x is near zero\n",
    "- Dynamic C/JAX/Numba code generation - Evaluate expressions faster\n",
    "\n",
    "PyTensor is based on Theano, which has been powering large-scale computationally intensive scientific investigations since 2007."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A simple example\n",
    "\n",
    "To begin, we define some pytensor tensors and show how to perform some basic operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytensor\n",
    "import pytensor.tensor as pt\n",
    "import numpy as np\n",
    "\n",
    "x = pt.scalar(name=\"x\")\n",
    "y = pt.vector(name=\"y\")\n",
    "\n",
    "print(\n",
    "    f\"\"\"\n",
    "x type: {x.type}\n",
    "x name = {x.name}\n",
    "---\n",
    "y type: {y.type}\n",
    "y name = {y.name}\n",
    "\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have defined the `x` and `y` tensors, we can create a new one by adding them together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = x + y\n",
    "z.name = \"x + y\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make the computation a bit more complex let us take the logarithm of the resulting tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = pt.log(z)\n",
    "w.name = \"log(x + y)\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the `pytensor.dprint` function to print the computational graph of any given tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pytensor.dprint(w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that this graph does not do any computation (yet!). It is simply defining the sequence of steps to be done. We can use `pytensor.function` to define a callable object so that we can push values trough the graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = pytensor.function(inputs=[x, y], outputs=w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the graph is compiled, we can push some concrete values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f(x=0, y=[1, np.e])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> TIP:\n",
    "> Sometimes we just want to debug, we can use `pytensor.graph.basic.Variable.eval` for that:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w.eval({x: 0, y: [1, np.e]})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can set intermediate values as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w.eval({z: [1, np.e]})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Graph optimization\n",
    "\n",
    "One of the most important features of `pytensor` is that it can automatically optimize the mathematical operations inside a graph. Let's consider a simple example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = pt.scalar(name=\"a\")\n",
    "b = pt.scalar(name=\"b\")\n",
    "\n",
    "c = a / b\n",
    "c.name = \"a / b\"\n",
    "\n",
    "pytensor.dprint(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let us multiply `b` times `c`. This should result in simply `a`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = b * c\n",
    "d.name = \"b * c\"\n",
    "\n",
    "pytensor.dprint(d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The graph shows the full computation, but once we compile it the operation becomes the identity on `a` as expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = pytensor.function(inputs=[a, b], outputs=d)\n",
    "\n",
    "pytensor.dprint(g)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is in an PyTensor graph?\n",
    "\n",
    "The following diagram shows the basic structure of an `pytensor` graph.\n",
    "\n",
    "![pytensor graph](images/apply.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can can make these concepts more tangible by explicitly indicating them in the first example from the section above. Let us compute the graph components for the tensor `z`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    f\"\"\"\n",
    "z type: {z.type}\n",
    "z name = {z.name}\n",
    "z owner = {z.owner}\n",
    "z owner inputs = {z.owner.inputs}\n",
    "z owner op = {z.owner.op}\n",
    "z owner output = {z.owner.outputs}\n",
    "\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example: Logistic regression\n",
    "\n",
    "Here is a non-trivial example, which uses PyTensor to estimate the parameters of a logistic regression model using gradient information. We will use a bioassay example as a test case.\n",
    "\n",
    "Gelman et al. (2003) present an example of an acute toxicity test, commonly performed on animals to estimate the toxicity of various compounds.\n",
    "\n",
    "In this dataset `log_dose` includes 4 levels of dosage, on the log scale, each administered to 5 rats during the experiment. The response variable is `death`, the number of positive responses to the dosage.\n",
    "\n",
    "The number of deaths can be modeled as a binomial response, with the probability of death being a linear function of dose:\n",
    "\n",
    "$$\\begin{aligned}\n",
    "y_i &\\sim \\text{Bin}(n_i, p_i) \\\\\n",
    "\\text{logit}(p_i) &= a + b x_i\n",
    "\\end{aligned}$$\n",
    "\n",
    "The common statistic of interest in such experiments is the **LD50**, the dosage at which the probability of death is 50%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "rng = np.random\n",
    "\n",
    "dose = np.array([-0.86, -0.3 , -0.05,  0.73])\n",
    "deaths = np.array([0, 1, 3, 5])\n",
    "training_steps = 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's declare our symbolic variables.\n",
    "\n",
    "This code introduces a few new concepts. The `shared` function constructs so-called shared variables. These are hybrid symbolic and non-symbolic variables whose value may be shared between multiple functions. Shared variables can be used in symbolic expressions just like the objects returned by `dmatrices` but they also have an internal value that defines the value taken by this symbolic variable in all the functions that use it. It is called a shared variable because its value is shared between many functions. The value can be accessed and modified by the `get_value()` and `set_value()` methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pt.vector(\"X\")\n",
    "Y = pt.vector(\"Y\")\n",
    "b = pytensor.shared(1., name=\"b\")\n",
    "a = pytensor.shared(0., name=\"a\")\n",
    "\n",
    "print(\"Initial model:\", a.get_value(), b.get_value())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... then construct the expression graph:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Probability that target = 1\n",
    "p_1 = 1 / (1 + pt.exp(-(a +X*b))) \n",
    "\n",
    "# The prediction threshold\n",
    "prediction = p_1 > 0.5         \n",
    "\n",
    "# Cross-entropy loss function\n",
    "xent = -Y * pt.log(p_1) - (5-Y) * pt.log(1-p_1) \n",
    "\n",
    "# The cost to minimize\n",
    "cost = xent.mean()      \n",
    "\n",
    "# Compute the gradient of the cost\n",
    "ga, gb = pt.grad(cost, [a, b])                  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compile PyTensor functions for training the model and predicting from it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "step = pytensor.shared(10., name='step')\n",
    "train = pytensor.function(\n",
    "          inputs=[X, Y],\n",
    "          outputs=[prediction, xent],\n",
    "          updates=((a, a - step * ga), (b, b - step * gb), (step, step * 0.99)),\n",
    ")\n",
    "predict = pytensor.function(inputs=[X], outputs=prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can train the model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(training_steps):\n",
    "    pred, err = train(dose, deaths)\n",
    "    \n",
    "a_value, b_value = a.get_value(), b.get_value()\n",
    "\n",
    "print(\"Final model:\", a_value, b_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "logit = lambda x: 1. / (1 + np.exp(-x))\n",
    "xvals = np.linspace(-1, 1)\n",
    "plt.plot(xvals, logit(b_value*xvals + a_value))\n",
    "plt.plot(dose, deaths/5., 'ro')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Graph manipulation 101\n",
    "\n",
    "Another interesting feature of PyTensor is the ability to manipulate the computational graph, something that is not possible with TensorFlow or PyTorch. Here we continue with the example above in order to illustrate the main idea around this technique."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get input tensors\n",
    "list(pytensor.graph.graph_inputs(graphs=[w]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a simple example, let's add an `pytensor.tensor.exp` before the `pytensor.tensor.log` (to get the identity function)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parent_of_w = w.owner.inputs[0]  # get z tensor\n",
    "new_parent_of_w = pt.exp(parent_of_w)  # modify the parent of w\n",
    "new_parent_of_w.name = \"exp(x + y)\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the graph of `w` has actually not changed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pytensor.dprint(w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To modify the graph we need to use the `pytensor.clone_replace` function, which *returns a copy of the initial subgraph with the corresponding substitutions.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_w = pytensor.clone_replace(output=[w], replace={parent_of_w: new_parent_of_w})[0]\n",
    "new_w.name = \"log(exp(x + y))\"\n",
    "pytensor.dprint(new_w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can test the modified graph by passing some input to the new graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_w.eval({x: 0, y: [1, np.e]})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, the new graph is just the identity function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> NOTE:\n",
    "> Again, note that `pytensor` is clever enough to omit the `exp` and `log` once we compile the function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = pytensor.function(inputs=[x, y], outputs=new_w)\n",
    "\n",
    "pytensor.dprint(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f(x=0, y=[1, np.e])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PyMC Variable Classes\n",
    "\n",
    "Bayesian inference begins with specification of a probability model relating unknown variables to data. PyMC provides the basic building blocks for Bayesian probability models: stochastic random variables, deterministic variables, and factor potentials. \n",
    "\n",
    "A **stochastic random variable** is a factor whose value is not completely determined by its parents, while the value of a **deterministic random variable** is entirely determined by its parents. Most models can be constructed using only these two variable types. The third quantity, the **factor potential**, is *not* a variable but simply a\n",
    "log-likelihood term or constraint that is added to the joint log-probability to modify it. \n",
    "\n",
    "### The Distribution class\n",
    "\n",
    "A stochastic variable is represented in PyMC by a `Distribution` class. This structure adds functionality to Pytensor's `pytensor.tensor.random.op.RandomVariable` class, mainly by registering it with an associated PyMC `Model`. As we demonstrated in a previous section, `Distribution` objects are only usable inside of a `Model` context.\n",
    "\n",
    "`Distribution` subclasses (i.e. implementations of specific statistical distributions) will accept several arguments when constructed:\n",
    "\n",
    "`name`\n",
    ":   Name for the new model variable. This argument is **required**, and is used as a label and index value for the variable.\n",
    "\n",
    "`shape`\n",
    ":   The variable's shape.\n",
    "\n",
    "`total_size`\n",
    ":   The overall size of the variable (this variable will not exist for scalars).\n",
    "\n",
    "`dims`\n",
    ":   A tuple of dimension names known to the model.\n",
    "\n",
    "`transform`\n",
    ":   A transformation to be applied to the distribution when used by the model, especially when the distribution is constrained.\n",
    "\n",
    "`initval`\n",
    ":   Numeric or symbolic untransformed initial value of matching shape, or one of the following initial value strategies: \"moment\", \"prior\". Depending on the sampler's settings, a random jitter may be added to numeric, symbolic or moment-based initial values in the transformed space.\n",
    "\n",
    "`model`\n",
    ":   The PyMC model to which the variable belongs.\n",
    "\n",
    "\n",
    "As we previewed in the introduction, `Distribution` has a classmethod `dist` that returns a **stateless** probability distribution of that type; that is, without being wrapped in a PyMC random variable object. Sometimes we wish to use a particular statistical distribution, without using it as a variable in a model; for example, to generate random numbers from the distribution. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import pymc as pm\n",
    "import arviz as az\n",
    "\n",
    "x = pm.Exponential.dist(1)\n",
    "samples = pm.draw(x, draws=1000)\n",
    "sns.histplot(samples);"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Motivating Example: Educational Outcomes for Hearing-impaired Children\n",
    "\n",
    "To demonstrate the PyMC API in-depth, we will use a dataset of educational outcomes for children with hearing impairment. Here, we are interested in determining factors that are associated with better or poorer learning outcomes. \n",
    "\n",
    "This anonomized dataset is taken from the Listening and Spoken Language Data Repository (LSL-DR), an international data repository that tracks the demographics and longitudinal outcomes for children who have hearing loss and are enrolled programs focused on supporting listening and spoken language development. Researchers are interesting in discovering factors related to improvements in educational outcomes at within these programs.\n",
    "\n",
    "There is a suite of available predictors, including: \n",
    "\n",
    "* gender (`male`)\n",
    "* number of siblings in the household (`siblings`)\n",
    "* index of family involvement (`family_inv`)\n",
    "* whether the primary household language is not English (`non_english`)\n",
    "* presence of a previous disability (`prev_disab`)\n",
    "* non-white race (`non_white`)\n",
    "* age at the time of testing (in months, `age_test`)\n",
    "* whether hearing loss is not severe (`non_severe_hl`)\n",
    "* whether the subject's mother obtained a high school diploma or better (`mother_hs`)\n",
    "* whether the hearing impairment was identified by 3 months of age (`early_ident`).\n",
    "\n",
    "The outcome variable is a standardized test score in one of several learning domains."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "\n",
    "test_scores = pl.read_csv(pm.get_data(\"test_scores.csv\"))\n",
    "test_scores.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "test_scores[\"score\"].plot.hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# Dropping missing values is a very bad idea in general, but we do so here for simplicity\n",
    "test_scores = test_scores.drop_nulls().cast(pl.Float32)\n",
    "y = test_scores[\"score\"]\n",
    "# Standardize the features\n",
    "X = test_scores.drop(\"score\").select((pl.all()-pl.all().mean()) / pl.all().std())\n",
    "\n",
    "N, D = X.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Model\n",
    "\n",
    "This is a **multivariate regression** model, which takes the form:\n",
    "\n",
    "$$\\begin{aligned} \n",
    "Y  &\\sim \\mathcal{N}(\\mu, \\sigma^2) \\\\\n",
    "\\mu &= \\beta_0 + X \\beta\n",
    "\\end{aligned}$$\n",
    "\n",
    "where $X$ is a matrix of predictors and $\\beta$ a vector of coefficients.\n",
    "\n",
    "However, while there are several potential predictors in the LSL-DR dataset, it is difficult *a priori* to determine which ones are relevant for constructing an effective statistical model. There are a number of approaches for conducting variable selection, but a popular automated method is *regularization*, whereby ineffective covariates are shrunk towards zero via regularization (a form of penalization) if they do not contribute to predicting outcomes. \n",
    "\n",
    "You may have heard of regularization from machine learning or classical statistics applications, where methods like the lasso or ridge regression shrink parameters towards zero by applying a penalty to the size of the regression parameters. In a Bayesian context, we apply an appropriate prior distribution to the regression coefficients. One such prior is the *hierarchical regularized horseshoe*, which uses two regularization strategies, one global and a set of local local parameters, one for each coefficient. The key to making this work is by selecting a long-tailed distribution as the shrinkage prior, which allows some to be nonzero, while pushing the rest towards zero.\n",
    "\n",
    "The horeshoe prior for each regression coefficient $\\beta_i$ looks like this:\n",
    "\n",
    "$$\\beta_i \\sim N\\left(0, \\tau^2 \\cdot \\tilde{\\lambda}_i^2\\right)$$\n",
    "\n",
    "where $\\sigma$ is the prior on the error standard deviation that will also be used for the model likelihood.\n",
    "\n",
    "Where $\\tau$ is the global shrinkage parameter and $\\tilde{\\lambda}_i$ is the local. Let's start global: for the prior on $\\tau$ we will use a Half-StudentT distribution, which is a reasonable choice becuase it is heavy-tailed.\n",
    "\n",
    "$$\n",
    "\\tau \\sim \\textrm{Half-StudentT}_{2} \\left(\\frac{D_0}{D - D_0} \\cdot \\frac{\\sigma}{\\sqrt{N}}\\right).\n",
    "$$\n",
    "\n",
    "One catch is that the parameterization of the prior requires a pre-specified value $D_0$, which represents the true number of non-zero coefficients. Fortunately, a reasonable guess at this value is all that is required, and it need only be within an order of magnitude of the true number. Let's use half the number of predictors as our guess:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "D0 = int(D / 2)\n",
    "D0"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Meanwhile, the local shrinkage parameters are defined by the ratio:\n",
    "\n",
    "$$\\tilde{\\lambda}_i^2 = \\frac{c^2 \\lambda_i^2}{c^2 + \\tau^2 \\lambda_i^2}.$$\n",
    "\n",
    "To complete this specification, we need priors on $\\lambda_i$ and $c$;  as with the global shrinkage, we use a long-tailed $\\textrm{Half-StudentT}_5(1)$  on the $\\lambda_i$. We need $c^2$ to be strictly positive, but not necessarily long-tailed, so an inverse gamma prior on $c^2$, $c^2 \\sim \\textrm{InverseGamma}(1, 1)$ fits the bill.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creation of stochastic random variables\n",
    "\n",
    "Stochastic random variables with standard distributions provided by PyMC can be created in a single line using special subclasses of the `Distribution` class. \n",
    "\n",
    "This model employs a couple of new distributions: the `HalfStudentT` distribution for the $\\tau$ and $\\lambda$ priors, and the `InverseGamma` distribution for the $c2$ variable.\n",
    "\n",
    "We are also going to take advantage of **named dimensions** in PyMC by passing the input variable names into the model as coordinates called \"predictors\". This will allow us to pass this vector of names as a replacement for the `shape` integer argument in the vector-valued parameters. The model will then associate the appropriate name with each latent parameter that it is estimating. This is a little more work to set up, but will pay dividends later when we are working with our model output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "X.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "with pm.Model(coords={\"predictors\": X.columns}) as test_score_model:\n",
    "\n",
    "    # Prior on error SD\n",
    "    sigma = pm.HalfNormal(\"sigma\", 25)\n",
    "    \n",
    "    # Global shrinkage prior\n",
    "    tau = pm.HalfStudentT(\"tau\", 2, D0 / (D - D0) * sigma / np.sqrt(N))\n",
    "    # Local shrinkage prior\n",
    "    lam = pm.HalfStudentT(\"lam\", 2, dims=\"predictors\")\n",
    "    c2 = pm.InverseGamma(\"c2\", 1, 0.1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Contexts and Random Variables\n",
    "\n",
    "As we have seen, the canonical way to specify PyMC models is using a `Model` context manager. Generally speaking, a context manager is a Python idiom that does the following:\n",
    "\n",
    "```python\n",
    "    VAR = EXPR\n",
    "    VAR.__enter__()\n",
    "    try:\n",
    "        USERCODE\n",
    "    finally:\n",
    "        VAR.__exit__()\n",
    "\n",
    "```\n",
    "\n",
    "As an analogy, `Model` is a tape machine that records what is being added to the model; it keeps track the random variables (observed or unobserved) and other model components. The model context then computes some simple model properties, builds a **bijection** mapping that transforms between Python dictionaries and numpy/Pytensor ndarrays. , More importantly, a `Model` contains methods to compile Pytensor functions that take Random Variables--that are also\n",
    "initialised within the same model--as input.\n",
    "\n",
    "Within a model context, random variables are essentially Pytensor `TensorVariables`:\n",
    "\n",
    "```python\n",
    "with pm.Model() as model:\n",
    "    z = pm.Normal('z', mu=0., sigma=5.)             # ==> pytensor.tensor.var.TensorVariable\n",
    "    x = pm.Normal('x', mu=z, sigma=1., observed=5.) # ==> pytensor.tensor.var.TensorVariable\n",
    "pm.logp(z, 2.5).eval()                              # ==> -2.65337645\n",
    "model.compile_logp()({'z': 2.5})                    # ==> -6.6973152\n",
    "\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyMC includes most of the probability density functions (for continuous variables) and probability mass functions (for discrete variables) used in statistical modeling. These distributions are divided into five distinct categories:\n",
    "\n",
    "* Univariate continuous\n",
    "* Univariate discrete\n",
    "* Multivariate\n",
    "* Mixture\n",
    "* Timeseries\n",
    "\n",
    "Probability distributions are all subclasses of `Distribution`, which in turn has two major subclasses: `Discrete` and `Continuous`. In terms of data types, a `Continuous` random variable is given whichever floating point type is defined by `pytensor.config.floatX`, while `Discrete` variables are given `int16` types when `pytensor.config.floatX` is `float32`, and `int64` otherwise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "sigma.dtype"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multivariate and Timeseries random variables are vector-valued, rather than scalar (though `Continuous` and `Discrete` variables may have non-scalar values)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "sigma.shape.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "lam.shape.eval()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All of the `Distribution` subclasses included in PyMC will have two key methods, `random()` and `logp()`, which are used to generate random values and compute the log-probability of a value, respectively.\n",
    "\n",
    "```python\n",
    "class SomeDistribution(Continuous):\n",
    "    def __init__(...):\n",
    "        ...\n",
    "\n",
    "    def random(self, point=None, size=None):\n",
    "        ...\n",
    "        return random_samples\n",
    "\n",
    "    def logp(self, value):\n",
    "        ...\n",
    "        return total_log_prob\n",
    "```\n",
    "\n",
    "PyMC expects the `logp()` method to return a log-probability evaluated at the passed `value` argument. This method is used internally by all of the inference methods to calculate the model log-probability that is used for fitting models. The `random()` method is used to simulate values from the variable, and is used internally for posterior predictive checks.\n",
    "\n",
    "Distributions will optionally have `cdf` and `icdf` methods, representing the cumulative distribution function and inverse cumulative distribution functions, respectively."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you have a well-behaved density function, we can use it in a model to build a model log-likelihood function. Almost any Pytensor function can be turned into a\n",
    "distribution using the `CustomDist` function. For exmaple, a uniformly-distributed discrete stochastic variable could be created manually from a function that computes its log-probability as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pm.Model():\n",
    "    \n",
    "    def uniform_logp(value, lower, upper):\n",
    "        return pm.math.switch((value > upper) | (value < lower), -np.inf, -pm.math.log(upper - lower + 1))\n",
    "\n",
    "    u = pm.CustomDist('u', 0, 10, logp=uniform_logp, dtype='int64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pm.logp(u, 6).eval()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Passing values outside the support of the distribution to `logp()` will return `-inf`, since the value has no probability. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pm.logp(u, -4).eval()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A couple of things to notice: while the function specified for the `logp` argument can be an arbitrary Python function, it must use **Pytensor operators and functions** in its body. This is because one or more of the arguments passed to the function may be `TensorVariables`, and they must be supported. \n",
    "\n",
    "To emphasize, the Python function passed to `CustomDist` should compute the *log*-density or *log*-probability of the variable. That is why the return value in the example above is `-log(upper-lower+1)` rather than `1/(upper-lower+1)`."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Auto-transformation\n",
    "\n",
    "To support efficient sampling by PyMC's MCMC algorithms, any continuous variables that are constrained to a sub-interval of the real line are automatically transformed so that their support is unconstrained. This frees sampling algorithms from having to deal with boundary constraints.\n",
    "\n",
    "For example, if we look at the variables we have create in the test score model so far:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_score_model.value_vars"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model's `value_vars` attribute stores the values of each random variable actually used by the model's log-likelihood.\n",
    "\n",
    "As the name suggests, the variables `sigma`, `tau`, `lam`, and `c2` have been log-transformed, and this is the space over which posterior sampling takes place. When a sample is drawn, the value of the transformed variable is simply back-transformed to recover the original variable.\n",
    "\n",
    "By default, auto-transformed variables are ignored when summarizing and plotting model output, since they are not generally of interest to the user."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Returning to our model specification, the intercept will not be subject to regularization, and will be given a normal distribution centered on the population mean:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with test_score_model:\n",
    "    beta0 = pm.Normal(\"beta0\", 100, 25)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deterministic Variables\n",
    "\n",
    "A deterministic variable is one whose values are **completely determined** by the values of their parents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with test_score_model:\n",
    "\n",
    "    shrinkage_sd = tau * lam * pm.math.sqrt(c2 / (c2 + tau**2 * lam**2))\n",
    "\n",
    "    beta = pm.Normal('beta', 0, shrinkage_sd, dims='predictors')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "so `shrinkage_sd`'s value can be computed exactly from the values of its parents `tau`, `lam` and `c2`.\n",
    "\n",
    "There are two types of deterministic variables in PyMC:\n",
    "\n",
    "#### Anonymous deterministic variables\n",
    "\n",
    "The easiest way to create a deterministic variable is to operate on or transform one or more variables in a model directly, as we have done above for `shrinkage_sd`.\n",
    "\n",
    "These are called *anonymous* variables because we did not wrap it with a call to `Determinstic`, which gives it a name as its first argument. We simply specified the variable as a Python (or, Pytensor) expression. This is therefore the simplest way to construct a determinstic variable. The only caveat is that the values generated by anonymous determinstics at every iteration of a MCMC algorithm, for example, are not recorded to the resulting trace. So, this approach is only appropriate for intermediate values in your model that you do not wish to obtain posterior estimates for, alongside the other variables in the model.\n",
    "\n",
    "#### Named deterministic variables\n",
    "\n",
    "To ensure that deterministic variables' values are accumulated during sampling, they should be instantiated using the **named deterministic** interface; this uses the `Deterministic` function to create the variable. Two things happen when a variable is created this way:\n",
    "\n",
    "1. The variable is given a name (passed as the first argument)\n",
    "2. The variable is appended to the model's list of random variables, which ensures that its values are tallied.\n",
    "\n",
    "If we wanted named variabels for the shrinkage standard deviation, we would have specified:\n",
    "\n",
    "```python\n",
    "shrinkage_sd = pm.Deterministic('shrinkage_sd', tau * lam * pm.math.sqrt(c2 / (c2 + tau**2 * lam**2)))\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Observed Random Variables\n",
    "\n",
    "Stochastic random variables whose values are observed (*i.e.* data likelihoods) are represented by a different class than unobserved random variables. A `ObservedRV` object is instantiated any time a stochastic variable is specified with data passed as the `observed` argument. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with test_score_model:\n",
    "    \n",
    "    scores = pm.Normal(\"scores\", beta0 + X.to_numpy() @ beta, sigma, observed=y.to_numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pm.model_to_graphviz(test_score_model)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An important responsibility of `ObservedRV` is to automatically handle missing values in the data, when they are present (absent?). More on this later."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Factor Potentials\n",
    "\n",
    "For some applications, we want to be able to modify the joint density by incorporating terms that don't correspond to probabilities of variables conditional on parents. For example, suppose in the coal mining disasters model we want to constrain the difference between the early and late means to be less than 1, so that the joint density becomes: \n",
    "\n",
    "$$p(y,\\tau,\\lambda_1,\\lambda_2) \\propto p(y|\\tau,\\lambda_1,\\lambda_2) p(\\tau) p(\\lambda_1) p(\\lambda_2) I(|\\lambda_2-\\lambda_1| \\lt 1)$$\n",
    "\n",
    "We call such log-probability terms **factor potentials** (Jordan 2004). Bayesian\n",
    "hierarchical notation doesn't accomodate these potentials. \n",
    "\n",
    "### Creation of Potentials\n",
    "\n",
    "A potential can be created via the `Potential` function, in a way very similar to `Deterministic`'s named interface:\n",
    "\n",
    "```python\n",
    "with disaster_model:\n",
    "    \n",
    "    rate_constraint = pm.Potential('rate_constraint', pm.math.switch(pm.math.abs(early_mean-late_mean)>1, -np.inf, 0))\n",
    "```\n",
    "\n",
    "The function takes just a `name` as its first argument and an expression returning the appropriate log-probability as the second argument.\n",
    "\n",
    "A common use of a factor potential is to represent an observed likelihood, where the **observations are partly a function of model variables**. In the contrived example below, we are representing the error in a linear regression model as a zero-mean normal random variable. Thus, the \"data\" in this scenario is the residual, which is a function both of the data and the regression parameters. \n",
    "\n",
    "If we represent this as a standard likelihood function (a `Distribution` with an `observed` keyword argument), we run into problems. This parameterization would not be compatible with an observed stochastic, because the `err` term would become fixed in the likelihood and not be allowed to change during sampling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_vals = np.array([15, 10, 16, 11, 9, 11, 10, 18, 11])\n",
    "x_vals = np.array([1, 2, 4, 5, 6, 8, 19, 18, 12])\n",
    "\n",
    "with pm.Model() as regression:\n",
    "\n",
    "    sigma = pm.HalfCauchy('sigma', 5)\n",
    "    beta = pm.Normal('beta', 0, sigma=2)\n",
    "    mu = pm.Normal('mu', 0, sigma=10)\n",
    "\n",
    "    err = y_vals - (mu + beta*x_vals)\n",
    "                  \n",
    "    like = pm.Normal('like', 0, sigma=sigma, observed=err)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead, we can re-express the likelihood as a factor potential, which is a function of the data and the model parameters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pm.Model() as regression:\n",
    "\n",
    "    sigma = pm.HalfCauchy('sigma', 5)\n",
    "    beta = pm.Normal('beta', 0, sigma=2)\n",
    "    mu = pm.Normal('mu', 0, sigma=10)\n",
    "\n",
    "    err = y_vals - (mu + beta*x_vals)\n",
    "                  \n",
    "    like = pm.Potential('like', \n",
    "        pm.logp(\n",
    "            pm.Normal.dist(0, sigma=sigma), \n",
    "            err\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise: Bioassay model\n",
    "\n",
    "Gelman et al. (2003) present an example of an acute toxicity test, commonly performed on animals to estimate the toxicity of various compounds.\n",
    "\n",
    "In this dataset `log_dose` includes 4 levels of dosage, on the log scale, each administered to 5 rats during the experiment. The response variable is death, the number of positive responses to the dosage.\n",
    "\n",
    "The number of deaths can be modeled as a binomial response, with the probability of death being a linear function of dose:\n",
    "\n",
    "$$\\begin{aligned}\n",
    "y_i &\\sim \\text{Bin}(n_i, p_i) \\\\\n",
    "\\text{logit}(p_i) &= a + b x_i\n",
    "\\end{aligned}$$\n",
    "\n",
    "The common statistic of interest in such experiments is the LD50, the dosage at which the probability of death is 50%.\n",
    "\n",
    "Specify this model in PyMC:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Log dose in each group\n",
    "log_dose = [-.86, -.3, -.05, .73]\n",
    "\n",
    "# Sample size in each group\n",
    "n = 5\n",
    "\n",
    "# Outcomes\n",
    "deaths = [0, 1, 3, 5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write model here"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## References\n",
    "\n",
    "1. Ching & Chen. 2007. Transitional Markov chain Monte Carlo method for Bayesian model updating, model class selection and model averaging. Journal of Engineering Mechanics 2007\n",
    "2.\tHoffman MD, Gelman A. 2014. The No-U-turn sampler: adaptively setting path lengths in Hamiltonian Monte Carlo. The Journal of Machine Learning Research. 15(1):1593-1623.\n",
    "3. M.I. Jordan. 2004. Graphical models. Statist. Sci., 19(1):140–155.\n",
    "4. Neal, R. M. 2003. Slice sampling. The Annals of Statistics, 31(3), 705–767. doi:10.1111/1467-9868.00198"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  },
  "latex_envs": {
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 0
  },
  "vscode": {
   "interpreter": {
    "hash": "485c2aecfeff35fe97c500045cb91db26354005e32990317d3834cb0213a269e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
