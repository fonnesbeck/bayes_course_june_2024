{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2b77a7e0",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/fonnesbeck/bayes_course_dec_2023/blob/master/notebooks/Section2_2-Model_Checking.ipynb)\n",
    "\n",
    "# MCMC Output Processing and Model Checking with ArviZ\n",
    "\n",
    "ArviZ is a Python package for exploratory analysis of Bayesian models. It includes functions for posterior analysis, model checking, comparison and diagnostics. ArviZ is designed to work with output from a wide range of Bayesian inference libraries, including PyMC, emcee, Stan, Pyro, and TensorFlow Probability.\n",
    "\n",
    "ArviZ is built on top of the popular libraries xarray and matplotlib. It is also built with the same design principles as PyMC, so if you are familiar with PyMC, you will find ArviZ easy to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ad18c10",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.stats as st\n",
    "import pymc as pm\n",
    "import pytensor.tensor as pt\n",
    "import arviz as az\n",
    "import warnings\n",
    "warnings.simplefilter(\"ignore\")\n",
    "\n",
    "RANDOM_SEED = 20090425"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2b728385",
   "metadata": {},
   "source": [
    "### Example: Effect of coaching on SAT scores\n",
    "\n",
    "This example was taken from Gelman *et al.* (2013):\n",
    "\n",
    "> A study was performed for the Educational Testing Service to analyze the effects of special coaching programs on test scores. Separate randomized experiments were performed to estimate the effects of coaching programs for the SAT-V (Scholastic Aptitude Test- Verbal) in each of eight high schools. The outcome variable in each study was the score on a special administration of the SAT-V, a standardized multiple choice test administered by the Educational Testing Service and used to help colleges make admissions decisions; the scores can vary between 200 and 800, with mean about 500 and standard deviation about 100. The SAT examinations are designed to be resistant to short-term efforts directed specifically toward improving performance on the test; instead they are designed to reflect knowledge acquired and abilities developed over many years of education. Nevertheless, each of the eight schools in this study considered its short-term coaching program to be successful at increasing SAT scores. Also, there was no prior reason to believe that any of the eight programs was more effective than any other or that some were more similar in effect to each other than to any other.\n",
    "\n",
    "We are given the estimated coaching effects (`y`) and their sampling variances (`s`). The estimates were obtained by independent experiments, with relatively large sample sizes (over thirty students in each school), so you it can be assumed that they have approximately normal sampling distributions with known variances variances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e87ed9ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.array([28, 8, -3, 7, -1, 1, 18, 12])\n",
    "s = np.array([15, 10, 16, 11, 9, 11, 10, 18])\n",
    "schools = np.array(\n",
    "    [\n",
    "        \"Choate\",\n",
    "        \"Deerfield\",\n",
    "        \"Phillips Andover\",\n",
    "        \"Phillips Exeter\",\n",
    "        \"Hotchkiss\",\n",
    "        \"Lawrenceville\",\n",
    "        \"St. Paul's\",\n",
    "        \"Mt. Hermon\",\n",
    "    ]\n",
    ")\n",
    "\n",
    "with pm.Model(coords={'school': schools}) as schools:\n",
    "    \n",
    "    mu = pm.Normal(\"mu\", 0, sigma=1e6)\n",
    "    tau = pm.HalfCauchy(\"tau\", 5)\n",
    "\n",
    "    theta = pm.Normal(\"theta\", mu, sigma=tau, dims='school')\n",
    "\n",
    "    obs = pm.Normal(\"obs\", theta, sigma=s, observed=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e441fca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "with schools:\n",
    "    # Model fitting\n",
    "    schools_trace = pm.sample(500, tune=0, random_seed=RANDOM_SEED)\n",
    "    # Posterior predictive sampling\n",
    "    pm.sample_posterior_predictive(schools_trace, extend_inferencedata=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a5926812",
   "metadata": {},
   "source": [
    "After running an MCMC simulation, `sample` returns an `arviz.InferenceData` object containing the samples for all the stochastic and named deterministic random variables. \n",
    "\n",
    "Data corresponding to each type of sampling is available as an `InferenceData` attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9697364a",
   "metadata": {},
   "outputs": [],
   "source": [
    "post = schools_trace.posterior\n",
    "post"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2c367359",
   "metadata": {},
   "source": [
    "Arbitary values can be added to an `InferenceData` object using the dictionary assignment syntax."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2e36be2",
   "metadata": {},
   "outputs": [],
   "source": [
    "post[\"log_tau\"] = np.log(post[\"tau\"])\n",
    "schools_trace.posterior"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "69a08f33",
   "metadata": {},
   "source": [
    "### Combining chains and draws\n",
    "\n",
    "`arviz.extract` is a convenience function aimed at taking care of the most common subsetting operations with MCMC samples. It can:\n",
    "- Combine chains and draws\n",
    "- Return a subset of variables (with optional filtering with regular expressions or string matching)\n",
    "- Return a subset of samples. Moreover by default it returns a random subset to prevent getting non-representative samples due to bad mixing.\n",
    "- Access any group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "504cd2db",
   "metadata": {},
   "outputs": [],
   "source": [
    "az.extract(post)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "71a3e265",
   "metadata": {},
   "source": [
    "If we only want a subset of the samples, we can use `extract` to resample them.\n",
    "\n",
    "> Use a random seed to get the same subset from multiple groups: `az.extract(idata, num_samples=100, rng=3)` and `az.extract(idata, group=\"log_likelihood\", num_samples=100, rng=3)` will continue to have matching samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae7b9dcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "az.extract(post, num_samples=100)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0bd20023",
   "metadata": {},
   "source": [
    "Sometimes we want to strip away the `InferenceData` overhead and access the raw values. We can extract values as a NumPy array using the `values` attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b2f69e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "az.extract(post).mu.values\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1e0e800f",
   "metadata": {},
   "source": [
    "Computing the mean and other summary statistics can be perfomed directly. By default, this will summarize across all dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16519af3",
   "metadata": {},
   "outputs": [],
   "source": [
    "post.mean()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "27383861",
   "metadata": {},
   "source": [
    "Often, however, we may just want to summarize across chains and draws, especially for multivariate quantities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b640492",
   "metadata": {},
   "outputs": [],
   "source": [
    "post.theta.mean(dim=[\"chain\", \"draw\"])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "239e0dbd",
   "metadata": {},
   "source": [
    "Finally, subsets of the output can be extracted using the `sel` method. For example, let's look at the posterior distribution of the coaching effects for just one school:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "094a3688",
   "metadata": {},
   "outputs": [],
   "source": [
    "post[\"theta\"].sel(school=\"Choate\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6f5fc4ea",
   "metadata": {},
   "source": [
    "Or, we can extract a single chain, for all variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "218473df",
   "metadata": {},
   "outputs": [],
   "source": [
    "post.sel(chain=0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3766302c",
   "metadata": {},
   "source": [
    "## Model Checking\n",
    "\n",
    "The final step in Bayesian computation is model checking, in order to ensure that inferences derived from your sample are valid\n",
    "\n",
    "There are **two components** to model checking:\n",
    "\n",
    "1. Convergence diagnostics\n",
    "2. Goodness of fit\n",
    "\n",
    "Convergence diagnostics are intended to detect **lack of convergence** in the Markov chain Monte Carlo sample; it is used to ensure that you have not halted your sampling too early. However, a converged model is not guaranteed to be a good model. \n",
    "\n",
    "The second component of model checking, goodness of fit, is used to check the **internal validity** of the model, by comparing predictions from the model to the data used to fit the model. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "09e109c8",
   "metadata": {},
   "source": [
    "## Convergence Diagnostics\n",
    "\n",
    "Valid inferences from sequences of MCMC samples are based on the\n",
    "assumption that the samples are derived from the true posterior\n",
    "distribution of interest. Theory guarantees this condition as the number\n",
    "of iterations approaches infinity. It is important, therefore, to\n",
    "determine the **minimum number of samples** required to ensure a reasonable\n",
    "approximation to the target posterior density. Unfortunately, no\n",
    "universal threshold exists across all problems, so convergence must be\n",
    "assessed independently each time MCMC estimation is performed. The\n",
    "procedures for verifying convergence are collectively known as\n",
    "*convergence diagnostics*.\n",
    "\n",
    "There are a handful of easy-to-use methods for checking convergence. Since you cannot prove convergence, but only show lack of convergence, there is no single method that is foolproof. So, its best to look at a suite of diagnostics together. \n",
    "\n",
    "We will cover the canonical set of checks:\n",
    "\n",
    "- Sampler statistics\n",
    "- Variable plotting\n",
    "- Divergences\n",
    "- R-hat\n",
    "- Effective Sample Size\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5ebee6d0",
   "metadata": {},
   "source": [
    "## Sampler Statistics\n",
    "\n",
    "When checking for convergence or when debugging a badly behaving sampler, it is often helpful to take a closer look at what the sampler is doing. For this purpose some samplers export statistics for each generated sample.\n",
    "\n",
    "NUTS provides several metrics related to the performance of the sampler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ea4bdd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "schools_trace.sample_stats"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0975f21d",
   "metadata": {},
   "source": [
    "The sample statistics variables are defined as follows:\n",
    "\n",
    "- `process_time_diff`: The time it took to draw the sample, as defined by the python standard library time.process_time. This counts all the CPU time, including worker processes in BLAS and OpenMP.\n",
    "\n",
    "- `step_size`: The current integration step size.\n",
    "\n",
    "- `diverging`: (boolean) Indicates the presence of leapfrog transitions with large energy deviation from starting and subsequent termination of the trajectory. “large” is defined as `max_energy_error` going over a threshold.\n",
    "\n",
    "- `lp`: The joint log posterior density for the model (up to an additive constant).\n",
    "\n",
    "- `energy`: The value of the Hamiltonian energy for the accepted proposal (up to an additive constant).\n",
    "\n",
    "- `energy_error`: The difference in the Hamiltonian energy between the initial point and the accepted proposal.\n",
    "\n",
    "- `perf_counter_diff`: The time it took to draw the sample, as defined by the python standard library time.perf_counter (wall time).\n",
    "\n",
    "- `perf_counter_start`: The value of time.perf_counter at the beginning of the computation of the draw.\n",
    "\n",
    "- `n_steps`: The number of leapfrog steps computed. It is related to `tree_depth` with `n_steps <= 2^tree_dept`.\n",
    "\n",
    "- `max_energy_error`: The maximum absolute difference in Hamiltonian energy between the initial point and all possible samples in the proposed tree.\n",
    "\n",
    "- `acceptance_rate`: The average acceptance probabilities of all possible samples in the proposed tree.\n",
    "\n",
    "- `step_size_bar`: The current best known step-size. After the tuning samples, the step size is set to this value. This should converge during tuning.\n",
    "\n",
    "- `tree_depth`: The number of tree doublings in the balanced binary tree."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1e307b36",
   "metadata": {},
   "source": [
    "It can be helpful to plot some of these variables, rather than staring at vectors of numbers!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ca95818",
   "metadata": {},
   "outputs": [],
   "source": [
    "schools_trace.sample_stats[\"tree_depth\"].plot(col=\"chain\", ls=\"none\", marker=\".\", alpha=0.3);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6998322a",
   "metadata": {},
   "outputs": [],
   "source": [
    "schools_trace.sample_stats[\"acceptance_rate\"].plot.hist(bins=20, density=True);"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "852b2108",
   "metadata": {},
   "source": [
    "## Output Visualization with ArviZ\n",
    "\n",
    "[ArviZ](https://arviz-devs.github.io/arviz/) is a Python package for exploratory analysis of Bayesian models. It includes functions for posterior analysis, model checking, comparison and diagnostics and is desingefd to work with a range of Bayesian inference libraries (not just PyMC).\n",
    "\n",
    "ArviZ is built on top of the popular libraries xarray and matplotlib. It is also built with the same design principles as PyMC, so if you are familiar with PyMC, you will find ArviZ easy to use.\n",
    "\n",
    "ArviZ accomodates many different input data types, even simple NumPy arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26abb6f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "az.plot_posterior(np.random.randn(100_000));"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6c439fef",
   "metadata": {},
   "source": [
    "Plotting a dictionary of arrays, ArviZ will interpret each key as the name of a different random variable. Each row of an array is treated as an independent series of draws from the variable, called a _chain_. Below, we have 10 chains of 50 draws, each for four different distributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d16245e",
   "metadata": {},
   "outputs": [],
   "source": [
    "size = (10, 50)\n",
    "az.plot_forest(\n",
    "    {\n",
    "        \"normal\": np.random.randn(*size),\n",
    "        \"gumbel\": np.random.gumbel(size=size),\n",
    "        \"student t\": np.random.standard_t(df=6, size=size),\n",
    "        \"exponential\": np.random.exponential(size=size),\n",
    "    }, \n",
    ");"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "db73769a",
   "metadata": {},
   "source": [
    "## Traceplot \n",
    "\n",
    "Perhaps the most-used ArviZ plot is the traceplot, obtained via the `plot_trace` function. This is a simple plot that is a good quick check to make sure nothing is obviously wrong, and is usually the first diagnostic step you will take. You've seen these already: just the time series of samples for an individual variable.\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "84bf7947",
   "metadata": {},
   "source": [
    "The `plot_trace` function from ArViZ by default generates a kernel density plot and a trace plot, with a different color for each chain of the simulation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "357a7634",
   "metadata": {},
   "outputs": [],
   "source": [
    "az.plot_trace(schools_trace, var_names=['mu', 'tau']);\n",
    "plt.tight_layout();"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2e2014b1",
   "metadata": {},
   "source": [
    "This sample is deliberately inadequate. Looking at the trace plot, the problems should be apparent.\n",
    "\n",
    "Can you identify the issues, based on what you learned in the previous section?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "cadb1f60",
   "metadata": {},
   "source": [
    "### Exercise: Take a quiz!\n",
    "\n",
    "[See how well you can identify sampling problems by looking at their traceplots](https://canyon289.github.io/bayesian-model-evaluation/lessonplans/mcmc_basics/#/14)\n",
    "\n",
    "The slides will show you a trace, and you have to guess whether the sampling is from one of:\n",
    "\n",
    "- MCMC with step size too small\n",
    "- MCMC with step size too large\n",
    "- MCMC with adequate step size\n",
    "- Independent samples from distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad6ec6b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "az.plot_pair(\n",
    "    schools_trace.posterior.theta,\n",
    "    coords={\"school\": [\"Choate\", \"Deerfield\", \"Phillips Andover\"]},\n",
    ");"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "99b772c1",
   "metadata": {},
   "source": [
    "## Divergences\n",
    "\n",
    "As we have seen, Hamiltonian Monte Carlo (and NUTS) performs numerical integration in order to explore the posterior distribution of a model. When the integration goes wrong, it can go dramatically wrong. \n",
    "\n",
    "For example, here are some Hamiltonian trajectories on the distribution of two correlated variables. Can you spot the divergent path?\n",
    "\n",
    "![divering HMC](images/diverging_hmc.png)\n",
    "\n",
    "The reason that this happens is that there may be parts of the posterior which are **hard to explore** for geometric reasons. Two ways of solving divergences are\n",
    "\n",
    "1. **Set a higher \"target accept\" rate**: Similarly (but not the same) as for Metropolis-Hastings, larger integrator steps lead to lower acceptance rates. A higher `target_accept` will generally cause a smaller step size, and more accurate integration.\n",
    "2. **Reparametrize**: If you can write your model in a different way that has the same joint probability density, you might do thpt. A lot of work is being done to automate this, since it requires careful work, and one goal of a probabilistic programming language is to iterate quickly. See [Hoffmann, Johnson, Tran (2018)](https://arxiv.org/abs/1811.11926), [Gorinova, Moore, Hoffmann (2019)](https://arxiv.org/abs/1906.03028).\n",
    "\n",
    "You should be wary of a trace that contains many divergences (particularly those clustered in particular regions of the parameter space), and give thought to how to fix them."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5d1881e6",
   "metadata": {},
   "source": [
    "### Divergence example\n",
    "\n",
    "The trajectories above are from a famous example of a difficult geometry: Neal's funnel. It is problematic because the geometry is very different in some regions of the state space relative to others. Specifically, for hierarchical models, as the scale parameter changes in size so do the values of the parameters it is constraining. When the variance is close to zero, the parameter space is very constrained relative to the majority of the support."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fce8f53a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def neals_funnel(dims=1):\n",
    "    with pm.Model() as funnel:\n",
    "        v = pm.Normal('v', 0, 3)\n",
    "        x_vec = pm.MvNormal('x_vec', mu=pt.zeros(dims), cov=2 * pt.exp(v) * pt.eye(dims), shape=dims)\n",
    "    return funnel\n",
    "\n",
    "with neals_funnel():\n",
    "    funnel_trace = pm.sample(random_seed=RANDOM_SEED)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "faf82bf7",
   "metadata": {},
   "source": [
    "PyMC provides us feedback on divergences, including a count and a recommendation on how to address them. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90b12a8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "funnel_trace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9e9f753",
   "metadata": {},
   "outputs": [],
   "source": [
    "diverging_ind = funnel_trace.sample_stats['diverging'].values[0].nonzero()\n",
    "diverging_ind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "787d699c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = az.plot_pair(funnel_trace)\n",
    "ax.plot(funnel_trace.posterior['v'].sel(chain=0).values[diverging_ind], funnel_trace.posterior['x_vec'].sel(chain=0).values[diverging_ind].squeeze(), 'y.');"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3c3dc86b",
   "metadata": {},
   "source": [
    "The `plot_parallel` function in the ArViZ library is a convenient way to identify patterns in divergent traces:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a21fcbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "az.plot_parallel(schools_trace);"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e60843d1",
   "metadata": {},
   "source": [
    "We have already seen this phenomenon in the radon example from the previous section section. Let's re-run the random-slopes model, which has a hierarchical model for the basement effect or radon measurements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f700540",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import radon data\n",
    "radon_data = pd.read_csv('../data/radon.csv', index_col=0)\n",
    "\n",
    "counties = radon_data.county.unique()\n",
    "n_counties = counties.shape[0]\n",
    "county = radon_data.county_code.values\n",
    "log_radon = radon_data.log_radon.values\n",
    "floor_measure = radon_data.floor.values\n",
    "log_uranium = np.log(radon_data.Uppm.values)\n",
    "county_lookup = dict(zip(counties, np.arange(n_counties)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eb436ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "with pm.Model() as varying_slope:\n",
    "    \n",
    "    # Priors\n",
    "    mu_b = pm.Normal('mu_b', mu=0., sigma=10)\n",
    "    sigma_b = pm.HalfCauchy('sigma_b', 5)\n",
    "    \n",
    "    # Common intercepts\n",
    "    a = pm.Normal('a', mu=0., sigma=10)\n",
    "    # Random slopes\n",
    "    b = pm.Normal('b', mu=mu_b, sigma=sigma_b, shape=n_counties)\n",
    "    \n",
    "    # Model error\n",
    "    sigma_y = pm.HalfCauchy('sigma_y',5)\n",
    "    \n",
    "    # Expected value\n",
    "    y_hat = a + b[county] * floor_measure\n",
    "    \n",
    "    # Data likelihood\n",
    "    y_like = pm.Normal('y_like', mu=y_hat, sigma=sigma_y, observed=log_radon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66c5e1b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "with varying_slope:\n",
    "    varying_slope_trace = pm.sample(cores=2, random_seed=RANDOM_SEED)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f1ab16c0",
   "metadata": {},
   "source": [
    "If we plot the locations of the divergences, we can see that they are located near the tip of the funnel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc313233",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = pd.Series(varying_slope_trace.posterior['b'].sel(chain=0)[:, 10], name='slope')\n",
    "y = pd.Series(varying_slope_trace.posterior['sigma_b'].sel(chain=0), name='slope group variance')\n",
    "diverging = varying_slope_trace.sample_stats['diverging'].sel(chain=0)\n",
    "\n",
    "jp = sns.jointplot(x=x, y=y, ylim=(0, .7), alpha=0.3)\n",
    "jp.ax_joint.plot(x.values[diverging], y.values[diverging], 'yo');"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "859ea9ed",
   "metadata": {},
   "source": [
    "When the group variance is small, this implies that the individual random slopes are themselves close to the group mean. In itself, this is not a problem, since this is the behavior we expect. However, if the sampler is tuned for the wider (unconstrained) part of the parameter space, it has trouble in the areas of higher curvature. The consequence of this is that the neighborhood close to the lower bound of $\\sigma_b$ is sampled poorly; indeed, in our chain it is not sampled at all below 0.1. The result of this will be biased inference."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "64624526",
   "metadata": {},
   "source": [
    "Now that we've spotted the problem, what can we do about it? The best way to deal with this issue is to reparameterize our model."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5973fd2e",
   "metadata": {},
   "source": [
    "### Solution: Non-centered Parameterization\n",
    "\n",
    "As we saw in the previous section, this is due to the use of a **centered** parameterization of the slope random effect. That is, the individual county effects are distributed around a county mean, with a spread controlled by the hierarchical standard deviation parameter. \n",
    "\n",
    "Here is the DAG of this centered model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e2784cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "pm.model_to_graphviz(varying_slope)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "cea0ae58",
   "metadata": {},
   "source": [
    "We can remove the issue with sampling geometry by **reparameterizing** our model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "127b3d0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "with pm.Model() as varying_slope_noncentered:\n",
    "    \n",
    "    # Priors\n",
    "    mu_b = pm.Normal('mu_b', mu=0., sigma=10)\n",
    "    sigma_b = pm.HalfCauchy('sigma_b', 5)\n",
    "    \n",
    "    # Common intercepts\n",
    "    a = pm.Normal('a', mu=0., sigma=10)\n",
    "    \n",
    "    # Non-centered random slopes\n",
    "    # Centered: b = Normal('b', mu_b, sigma=sigma_b, shape=counties)\n",
    "    z = pm.Normal('z', mu=0, sigma=1, shape=n_counties)\n",
    "    b = pm.Deterministic(\"b\", mu_b + z * sigma_b)\n",
    "    \n",
    "    # Model error\n",
    "    sigma_y =pm.HalfCauchy('sigma_y',5)\n",
    "    \n",
    "    # Expected value\n",
    "    y_hat = a + b[county] * floor_measure\n",
    "    \n",
    "    # Data likelihood\n",
    "    y_like = pm.Normal('y_like', mu=y_hat, sigma=sigma_y, observed=log_radon)\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8fc6e9df",
   "metadata": {},
   "source": [
    "This is a **non-centered** parameterization. By this, we mean that the random deviates are no longer explicitly modeled as being centered on $\\mu_b$. Instead, they are independent standard normals $\\upsilon$, which are then scaled by the appropriate value of $\\sigma_b$, before being location-transformed by the mean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f46182fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "pm.model_to_graphviz(varying_slope_noncentered)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "892a38af",
   "metadata": {},
   "source": [
    "This model samples much better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61b8153b",
   "metadata": {},
   "outputs": [],
   "source": [
    "with varying_slope_noncentered:\n",
    "    noncentered_trace = pm.sample(2000, tune=1000, cores=2, random_seed=RANDOM_SEED)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4ce6d6f0",
   "metadata": {},
   "source": [
    "The non-centered parameterization can fully explore the support of the posterior, and divergences are very rare."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f1f6a60",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = pd.Series(noncentered_trace.posterior['b'].sel(chain=0)[:, 75], name='slope')\n",
    "y = pd.Series(noncentered_trace.posterior['sigma_b'].sel(chain=0), name='slope group variance')\n",
    "\n",
    "sns.jointplot(x=x, y=y, ylim=(0, .7));"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6684646b",
   "metadata": {},
   "source": [
    "## Potential Scale Reduction: $\\hat{R}$\n",
    "\n",
    "Roughly, $\\hat{R}$ (*R-Hat*, or the *Gelman-Rubin statistic*) is the ratio of between-chain variance to within-chain variance. This diagnostic uses multiple chains to\n",
    "check for lack of convergence, and is based on the notion that if\n",
    "multiple chains have converged, by definition they should appear very\n",
    "similar to one another; if not, one or more of the chains has failed to\n",
    "converge.\n",
    "\n",
    "$\\hat{R}$ uses an analysis of variance approach to\n",
    "assessing convergence. That is, it calculates both the between-chain\n",
    "varaince (B) and within-chain varaince (W), and assesses whether they\n",
    "are different enough to worry about convergence. Assuming $m$ chains,\n",
    "each of length $n$, quantities are calculated by:\n",
    "\n",
    "$$\\begin{align}B &= \\frac{n}{m-1} \\sum_{j=1}^m (\\bar{\\theta}_{.j} - \\bar{\\theta}_{..})^2 \\\\\n",
    "W &= \\frac{1}{m} \\sum_{j=1}^m \\left[ \\frac{1}{n-1} \\sum_{i=1}^n (\\theta_{ij} - \\bar{\\theta}_{.j})^2 \\right]\n",
    "\\end{align}$$\n",
    "\n",
    "for each scalar estimand $\\theta$. Using these values, an estimate of\n",
    "the marginal posterior variance of $\\theta$ can be calculated:\n",
    "\n",
    "$$\\hat{\\text{Var}}(\\theta | y) = \\frac{n-1}{n} W + \\frac{1}{n} B$$\n",
    "\n",
    "Assuming $\\theta$ was initialized to arbitrary starting points in each\n",
    "chain, this quantity will overestimate the true marginal posterior\n",
    "variance. At the same time, $W$ will tend to underestimate the\n",
    "within-chain variance early in the sampling run. However, in the limit\n",
    "as $n \\rightarrow \n",
    "\\infty$, both quantities will converge to the true variance of $\\theta$.\n",
    "In light of this, $\\hat{R}$ monitors convergence using\n",
    "the ratio:\n",
    "\n",
    "$$\\hat{R} = \\sqrt{\\frac{\\hat{\\text{Var}}(\\theta | y)}{W}}$$\n",
    "\n",
    "This is called the **potential scale reduction**, since it is an estimate of\n",
    "the potential reduction in the scale of $\\theta$ as the number of\n",
    "simulations tends to infinity. In practice, we look for values of\n",
    "$\\hat{R}$ close to one (say, less than 1.1) to be confident that a\n",
    "particular estimand has converged. \n",
    "\n",
    "In ArViZ, the `summary` table, or a `plot_forest` with the `r_hat` flag set, will calculate $\\hat{R}$ for each stochastic node in the trace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "878b192a",
   "metadata": {},
   "outputs": [],
   "source": [
    "az.summary(schools_trace)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f5ccd261",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "\n",
    "Clearly the model above has not yet converged (we only ran it for 100 iterations without tuning, after all). Try running the `schools` model for a larger number of iterations, and see when $\\hat{R}$ converges to 1.0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "582103bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "with schools:\n",
    "    trace = pm.sample(1000, tune=2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afbbd0e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "az.summary(trace)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "dce73e45",
   "metadata": {},
   "source": [
    "## Effective Sample Size\n",
    "\n",
    "In general, samples drawn from MCMC algorithms will be autocorrelated. Unless the autocorrelation is very severe, this is not a big deal, other than the fact that autocorrelated chains may require longer sampling in order to adequately characterize posterior quantities of interest. The calculation of autocorrelation is performed for each lag $i=1,2,\\ldots,k$ (the correlation at lag 0 is, of course, 1) by: \n",
    "\n",
    "$$\\hat{\\rho}_i = 1 - \\frac{V_i}{2\\hat{\\text{Var}}(\\theta | y)}$$\n",
    "\n",
    "where $\\hat{\\text{Var}}(\\theta | y)$ is the same estimated variance as calculated for the Gelman-Rubin statistic, and $V_i$ is the variogram at lag $i$ for $\\theta$:\n",
    "\n",
    "$$\\text{V}_i = \\frac{1}{m(n-i)}\\sum_{j=1}^m \\sum_{k=i+1}^n (\\theta_{jk} - \\theta_{j(k-i)})^2$$\n",
    "\n",
    "This autocorrelation can be visualized using the `plot_autocorr` function in ArViZ:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8deb5dc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "az.plot_autocorr(schools_trace, var_names=['mu', 'tau'], combined=True);"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "11f79891",
   "metadata": {},
   "source": [
    "You can see very severe autocorrelation in `mu`, which is not surprising given the trace that we observed earlier."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "06d87525",
   "metadata": {},
   "source": [
    "The amount of correlation in an MCMC sample influences the **effective sample size** (ESS) of the sample. The ESS estimates how many *independent* draws contain the same amount of information as the *dependent* sample obtained by MCMC sampling.\n",
    "\n",
    "Given a series of samples $x_j$, the empirical mean is\n",
    "\n",
    "$$\n",
    "\\hat{\\mu} = \\frac{1}{n}\\sum_{j=1}^n x_j\n",
    "$$\n",
    "\n",
    "and the variance of the estimate of the empirical mean is \n",
    "\n",
    "$$\n",
    "\\operatorname{Var}(\\hat{\\mu}) = \\frac{\\sigma^2}{n},\n",
    "$$\n",
    "where $\\sigma^2$ is the true variance of the underlying distribution.\n",
    "\n",
    "Then the effective sample size is defined as the denominator that makes this relationship still be true:\n",
    "\n",
    "$$\n",
    "\\operatorname{Var}(\\hat{\\mu}) = \\frac{\\sigma^2}{n_{\\text{eff}}}.\n",
    "$$\n",
    "\n",
    "The effective sample size is estimated using the partial sum:\n",
    "\n",
    "$$\\hat{n}_{eff} = \\frac{mn}{1 + 2\\sum_{i=1}^T \\hat{\\rho}_i}$$\n",
    "\n",
    "where $T$ is the first odd integer such that $\\hat{\\rho}_{T+1} + \\hat{\\rho}_{T+2}$ is negative.\n",
    "\n",
    "The issue here is related to the fact that we are **estimating** the effective sample size from the fit output. Values of $n_{eff} / n_{iter} < 0.001$ indicate a biased estimator, resulting in an overestimate of the true effective sample size."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0da9ffe5",
   "metadata": {},
   "source": [
    "Vehtari *et al* (2019) recommend an ESS of at least 400 to ensure reliable estimates of variances and autocorrelations. They also suggest running at least 4 chains before calculating any diagnostics.\n",
    "\n",
    "Its important to note that ESS can vary across the quantiles of the MCMC chain being sampled. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e0ec7de",
   "metadata": {},
   "outputs": [],
   "source": [
    "az.plot_ess(schools_trace, var_names=['mu'])\n",
    "plt.tight_layout();"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3c03de3f",
   "metadata": {},
   "source": [
    "Using ArViZ, we can visualize the evolution of ESS as the MCMC sample accumulates. When the model is converging properly, both lines in this plot should be approximately linear.\n",
    "\n",
    "The standard ESS estimate, which mainly assesses how well the centre of the distribution is resolved, is referred to as **bulk-ESS**. In order to estimate intervals reliably, it is also important to consider the **tail-ESS**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13d6dbeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "az.plot_ess(schools_trace, var_names=['mu'], kind='evolution');"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4ff28e84",
   "metadata": {},
   "source": [
    "ESS statistics can also be tabulated, by generating a `summary` of the parameters of interest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df3c8e05",
   "metadata": {},
   "outputs": [],
   "source": [
    "az.summary(trace)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "74c960da",
   "metadata": {},
   "source": [
    "It is tempting to want to **thin** the chain to eliminate the autocorrelation (*e.g.* taking every 20th sample from traces with autocorrelation as high as 20), but this is a waste of time. Since thinning deliberately throws out the majority of the samples, no efficiency is gained; you ultimately require more samples to achive a particular desired sample size. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "fe0c0d57",
   "metadata": {},
   "source": [
    "## Bayesian Fraction of Missing Information\n",
    "\n",
    "The Bayesian fraction of missing information (BFMI) is a measure of how hard it is to\n",
    "sample level sets of the posterior at each iteration. Specifically, it quantifies **how well momentum resampling matches the marginal energy distribution**. \n",
    "\n",
    "$$\\text{BFMI} = \\frac{\\mathbb{E}_{\\pi}[\\text{Var}_{\\pi_{E|q}}(E|q)]}{\\text{Var}_{\\pi_{E}}(E)}$$\n",
    "\n",
    "$$\\widehat{\\text{BFMI}} = \\frac{\\sum_{i=1}^N (E_n - E_{n-1})^2}{\\sum_{i=1}^N (E_n - \\bar{E})^2}$$\n",
    "\n",
    "A small value indicates that the adaptation phase of the sampler was unsuccessful, and invoking the central limit theorem may not be valid. It indicates whether the sampler is able to *efficiently* explore the posterior distribution.\n",
    "\n",
    "Though there is not an established rule of thumb for an adequate threshold, values close to one are optimal. Reparameterizing the model is sometimes helpful for improving this statistic."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "65b5e6a3",
   "metadata": {},
   "source": [
    "BFMI calculation is only available in samples that were simulated using HMC or NUTS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ec92b80",
   "metadata": {},
   "outputs": [],
   "source": [
    "az.bfmi(trace)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7cf60f2b",
   "metadata": {},
   "source": [
    "Another way of diagnosting this phenomenon is by comparing the overall distribution of \n",
    "energy levels with the *change* of energy between successive samples. Ideally, they should be very similar.\n",
    "\n",
    "If the distribution of energy transitions is narrow relative to the marginal energy distribution, this is a sign of inefficient sampling, as many transitions are required to completely explore the posterior. On the other hand, if the energy transition distribution is similar to that of the marginal energy, this is evidence of efficient sampling, resulting in near-independent samples from the posterior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9b261c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "az.plot_energy(trace);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df47d191",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.array([28, 8, -3, 7, -1, 1, 18, 12])\n",
    "s = np.array([15, 10, 16, 11, 9, 11, 10, 18])\n",
    "schools = np.array(\n",
    "    [\n",
    "        \"Choate\",\n",
    "        \"Deerfield\",\n",
    "        \"Phillips Andover\",\n",
    "        \"Phillips Exeter\",\n",
    "        \"Hotchkiss\",\n",
    "        \"Lawrenceville\",\n",
    "        \"St. Paul's\",\n",
    "        \"Mt. Hermon\",\n",
    "    ]\n",
    ")\n",
    "\n",
    "with pm.Model(coords={'school': schools}) as schools_uncentered:\n",
    "    \n",
    "    mu = pm.Normal(\"mu\", 0, sigma=1e6)\n",
    "    tau = pm.HalfCauchy(\"tau\", 5)\n",
    "\n",
    "    z = pm.Normal('z', dims='school')\n",
    "    theta = pm.Deterministic(\"theta\", mu + tau*z, dims='school')\n",
    "\n",
    "    obs = pm.Normal(\"obs\", theta, sigma=s, observed=y)\n",
    "\n",
    "    trace_uncentered = pm.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8d51aea",
   "metadata": {},
   "outputs": [],
   "source": [
    "az.plot_energy(trace_uncentered)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2c223701",
   "metadata": {},
   "source": [
    "## Goodness of Fit\n",
    "\n",
    "As noted at the beginning of this section, convergence diagnostics are only the first step in the evaluation\n",
    "of MCMC model outputs. It is possible for an entirely unsuitable model to converge, so additional steps are needed to ensure that the estimated model adequately fits the data. \n",
    "\n",
    "One intuitive way of evaluating model fit is to compare model predictions with the observations used to fit\n",
    "the model. In other words, the fitted model can be used to simulate data, and the distribution of the simulated data should resemble the distribution of the actual data.\n",
    "\n",
    "Fortunately, simulating data from the model is a natural component of the Bayesian modelling framework. Recall, from the discussion on prediction, the posterior predictive distribution:\n",
    "\n",
    "$$p(\\tilde{y}|y) = \\int p(\\tilde{y}|\\theta) f(\\theta|y) d\\theta$$\n",
    "\n",
    "Here, $\\tilde{y}$ represents some hypothetical new data that would be expected, taking into account the posterior uncertainty in the model parameters. \n",
    "\n",
    "Sampling from the posterior predictive distribution is easy in PyMC. The `sample_posterior_predictive` function draws posterior predictive samples from all of the observed variables in the model. Consider the PKU model, \n",
    "where IQ is modeled as a Gaussian random variable, which is thought to be influenced by blood Phe levels."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "fca88d9b",
   "metadata": {},
   "source": [
    "The posterior predictive distribution of deaths uses the same functional\n",
    "form as the data likelihood, in this case a binomial stochastic. Here is\n",
    "the corresponding sample from the posterior predictive distribution (we typically need very few samples relative to the MCMC sample):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61685abe",
   "metadata": {},
   "outputs": [],
   "source": [
    "with schools_uncentered:\n",
    "    pm.sample_posterior_predictive(trace_uncentered, extend_inferencedata=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0e3ce3b8",
   "metadata": {},
   "source": [
    "The degree to which simulated data correspond to observations can be evaluated visually. This allows for a qualitative comparison of model-based replicates and observations. If there is poor fit, the true value of the data may appear in the tails of the histogram of replicated data, while a good fit will tend to show the true data in high-probability regions of the posterior predictive distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec38ba94",
   "metadata": {},
   "outputs": [],
   "source": [
    "az.plot_ppc(trace_uncentered);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d93f71d",
   "metadata": {},
   "outputs": [],
   "source": [
    "az.plot_ppc(schools_trace, kind='cumulative');"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3bb3acdc",
   "metadata": {},
   "source": [
    "A quantitative approach is to calculate quantiles of each observed data point relative to the corresponding distribution of posterior-simulated values. For an adequate fit, there should not be severe peaks in the histogram near zero and one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc1a0e6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import percentileofscore\n",
    "\n",
    "plt.hist([np.round(percentileofscore(_x, _y)/100, 2) for _x,_y in zip(schools_trace.posterior_predictive['obs'].sel(chain=0), y)], bins=25);"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "05420bb2",
   "metadata": {
    "colab_type": "text",
    "id": "Cmyutcs6Uk9U"
   },
   "source": [
    "\n",
    "---\n",
    "\n",
    "## Reference\n",
    "\n",
    "Gelman, A., & Rubin, D. B. (1992). Inference from iterative simulation using multiple sequences. Statistical Science. A Review Journal of the Institute of Mathematical Statistics, 457–472.\n",
    "\n",
    "[Vehtari, Gelman, Simpson, Carpenter, Bürkner (2019)](https://arxiv.org/abs/1903.08008) Rank-normalization, folding, and localization: An improved $\\hat{R}$ for assessing convergence of MCMC\n",
    "\n",
    "\n",
    "[Gelman, A., Hwang, J., & Vehtari, A. (2014). Understanding predictive information criteria for Bayesian models. Statistics and Computing, 24(6), 997–1016.](http://doi.org/10.1007/s11222-013-9416-2)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.5 ('bayes_course')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "485c2aecfeff35fe97c500045cb91db26354005e32990317d3834cb0213a269e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
