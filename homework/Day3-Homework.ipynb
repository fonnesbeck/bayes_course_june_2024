{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 3: Homework Exercises\n",
    "\n",
    "This material provides some hands-on experience using the methods learned from the third day's material."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.stats as st\n",
    "import pymc as pm\n",
    "import pytensor.tensor as at\n",
    "import arviz as az\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise: Effects of coaching on SAT scores\n",
    "\n",
    "This example was taken from Gelman *et al.* (2013):\n",
    "\n",
    "> A study was performed for the Educational Testing Service to analyze the effects of special coaching programs on test scores. Separate randomized experiments were performed to estimate the effects of coaching programs for the SAT-V (Scholastic Aptitude Test- Verbal) in each of eight high schools. The outcome variable in each study was the score on a special administration of the SAT-V, a standardized multiple choice test administered by the Educational Testing Service and used to help colleges make admissions decisions; the scores can vary between 200 and 800, with mean about 500 and standard deviation about 100. The SAT examinations are designed to be resistant to short-term efforts directed specifically toward improving performance on the test; instead they are designed to reflect knowledge acquired and abilities developed over many years of education. Nevertheless, each of the eight schools in this study considered its short-term coaching program to be successful at increasing SAT scores. Also, there was no prior reason to believe that any of the eight programs was more effective than any other or that some were more similar in effect to each other than to any other.\n",
    "\n",
    "You are given the estimated coaching effects (`d`) and their sampling variances (`s`). The estimates were obtained by independent experiments, with relatively large sample sizes (over thirty students in each school), so you can assume that they have approximately normal sampling distributions with known variances variances.\n",
    "\n",
    "Here are the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "J = 8\n",
    "d = np.array([28.,  8., -3.,  7., -1.,  1., 18., 12.])\n",
    "s = np.array([15., 10., 16., 11.,  9., 11., 10., 18.])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Construct an appropriate model for estimating whether coaching effects are positive, using a **centered parameterization**, and then compare the diagnostics for this model to that from an **uncentered parameterization**.\n",
    "\n",
    "Finally, perform goodness-of-fit diagnostics on the better model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise: Car Price Prediction\n",
    "\n",
    "We will use a small subset of [this Kaggle dataset](https://www.kaggle.com/datasets/nehalbirla/vehicle-dataset-from-cardekho). This data is amenable to both multiple linear regression and hierarchical regression, you can start with the former and move on to the latter. The data has the selling price of multiple used cars, along with some other features such as year of production, kilometers driven, fuel type, car brand and model, etc.\n",
    "\n",
    "The full dataset has a lot of information, but we chose to crop it down to a simpler form that is easier for you to work with. Our reduced dataset has the following features:\n",
    "\n",
    "- Selling price in thousands of dollars\n",
    "- Natural log of the kilometers driven\n",
    "- Year of production\n",
    "- Number of seats\n",
    "- Brand name\n",
    "- Car name\n",
    "- Full name of the car (with special edition modifiers)\n",
    "- Kilometers per liter of fuel\n",
    "- Engine CC\n",
    "\n",
    "The goal of this exercise is to be able to **predict the selling price based on the rest of the features using linear regressions**. We propose that you do the following steps to do so:\n",
    "\n",
    "1. Do an explorative data analysis. Visualize the distribution of selling prices and how it relates to the other features. Look at how the distribution changes across brands. Try to reason whether it's appropriate to transform the observations in some way to ellicit a linear relationship with some features in the dataset.\n",
    "2. Assuming the observations are normally distributed, write a series of linear regression models to fit the dataset\n",
    "    a. Try a model with a single intercept parameter and do prior predictive analysis to determine appropriate priors.\n",
    "    b. Add linear terms that relate the model with the numerical features in the data. Try to reason about adequate priors for the slopes, and whether it helps to standardize the features or not.\n",
    "    c. Run inference on both models and explain what happens to the observational noise parameter.\n",
    "3. Add a hierarchical structure that depends on the car brand. Do prior predictive simulations to choose appropriate priors, and run inference. The hierarchical structure could be on the intercept term, the slopes or both.\n",
    "4. (HARD) Add a nested hierarchy term to the model's intercept. The first level of the hierarchy is the brand, the second layer is the car name. Explore what happens if you use the centered or the non-centered parametrization in the second layer of the hierarchy.\n",
    "5. Compare all of the above models using `arviz.compare`. Which one is the highest ranking predictive model?\n",
    "6. (VERY HARD, Optional and without a coded answer) This is for people that want to explore beyond what we did with a simple linear regression. All of the models we used assumed normally distributed observations. The best fitting model is unable to explain the observed distribution of prices, and the reason for this is that prices are not normally distributed around a mean. Prices are chosen as multiple of a thousand, and usually they are grouped into tiers. It's not unusual to have multiple cars sold at exactly the same price. This leads to heteroskedastic observations across brands and car names, and the assumption of a unique observational noise parameter breaks down. We don't have a concrete answer on what observational distribution should really underlie the observations. We want to invite you to try to think:\n",
    "    a. How could you have different observational noises while still assuming that the observations are normally distributed?\n",
    "    b. What kind of observational distribution could you use to account for the groupings of multiple selling prices at the same value, and then some other selling prices that have more dispersion? As a hint, try to look into mixture models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(os.path.join(\"..\", \"data\", \"reduced_car_data.csv\"), header=0)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(df, hue=\"brand\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your answer here"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.5 ('bayes_course')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  },
  "vscode": {
   "interpreter": {
    "hash": "485c2aecfeff35fe97c500045cb91db26354005e32990317d3834cb0213a269e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
